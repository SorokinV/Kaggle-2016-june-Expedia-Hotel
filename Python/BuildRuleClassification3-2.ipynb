{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import datetime\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables: dirs and files\n",
    "\n",
    "dirBase   = '..'\n",
    "\n",
    "dirData   = os.path.join(dirBase,'Data')\n",
    "dirPicle  = os.path.join(dirBase,'Data','Pickle')\n",
    "#dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-1')\n",
    "#dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-2')\n",
    "dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-3')\n",
    "dirResult = os.path.join(dirBase,'Result','Leak') \n",
    "\n",
    "fileTrain      = os.path.join(dirData,'trainExt.csv')\n",
    "fileValidation = os.path.join(dirData,'trainExtMin.csv')\n",
    "fileTest       = os.path.join(dirData,'testExt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "chunkSize = 4000000\n",
    "nClusters = 100\n",
    "nTop      = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rules field\n",
    "\n",
    "rule1  = ['user_location_city','orig_destination_distance']\n",
    "rule2  = ['srch_destination_id','hotel_country','hotel_market']\n",
    "rule3  = ['srch_destination_id']\n",
    "rule3b = ['srch_destination_id','dt0m']\n",
    "rule4  = ['hotel_country']\n",
    "\n",
    "ruleXtest    = ['cnt','is_booking']\n",
    "ruleXtrain   = ['hotel_cluster'] + ruleXtest\n",
    "\n",
    "\n",
    "ruleTrain    = rule1+rule2+[i for i in rule3b if i not in rule2]+ruleXtrain+['dt0y','dty']\n",
    "ruleTest     = rule1+rule2+[i for i in rule3b if i not in rule2]+ruleXtest+['id']\n",
    "ruleValidate = ruleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rule (data,list) :\n",
    "    df       = data[list].groupby(list).count().reset_index()\n",
    "    df['nn'] = df.index\n",
    "    ar       = np.zeros((df.shape[0],nClusters),dtype=np.int16)\n",
    "    return([df,ar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:14:49.871324 Read test datas \n",
      "(1385522, 100) (43831, 100) (40718, 100) (206, 100)\n",
      "2016-06-05 16:14:57.819651 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for test data (chunks) on one time if not in disk\n",
    "print(datetime.datetime.now(),'Read test datas ')\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3bTest0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "if not OK :\n",
    "\n",
    "    readerTest = pd.read_csv(fileTest,chunksize=3000000,usecols=ruleTest);\n",
    "    i = 0;\n",
    "    for chunk in readerTest :\n",
    "        i += 1; print(datetime.datetime.now(),'chunk :',i,chunk.shape)\n",
    "\n",
    "        rule1Test = rule(chunk,rule1)\n",
    "        rule2Test = rule(chunk,rule2)\n",
    "        rule3Test = rule(chunk,rule3)\n",
    "        rule3bTest= rule(chunk,rule3b)\n",
    "        rule4Test = rule(chunk,rule4)\n",
    "\n",
    "    del chunk\n",
    "    del readerTest\n",
    "    \n",
    "    rule1Test[0].to_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2Test[0].to_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3Test[0].to_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    rule3bTest[0].to_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "    rule4Test[0].to_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'),rule3Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'),rule3bTest[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "else :\n",
    "    rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    rule3bdf= pd.read_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "    rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "    \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "    rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "    \n",
    "    rule1Test = [rule1df,rule1ar]\n",
    "    rule2Test = [rule2df,rule2ar]\n",
    "    rule3Test = [rule3df,rule3ar]\n",
    "    rule3bTest= [rule3bdf,rule3bar]\n",
    "    rule4Test = [rule4df,rule4ar]\n",
    "    del rule1df,rule2df,rule3df,rule3bdf,rule4df\n",
    "    del rule1ar,rule2ar,rule3ar,rule3bar,rule4ar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "\n",
    "print(datetime.datetime.now(),'Done ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iGroup = 0\n",
    "def ruleApply (group,Arr) :\n",
    "    global iGroup\n",
    "    nn = group['nn'].iloc[0]; \n",
    "    if (iGroup%50000==0) : \n",
    "        print(datetime.datetime.now(),'iGroup =',iGroup,'nn =',nn, 'len(group) = ',group.shape)\n",
    "    iGroup  += 1\n",
    "    gr  = group[['hotel_cluster','cnt']].groupby(['hotel_cluster']).sum().reset_index()\n",
    "    Arr[nn][gr['hotel_cluster']] += gr['cnt']\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTop (a) :\n",
    "    b = pd.Series(a); b=np.array(b[b>0].nlargest(nTop).index); \n",
    "    b=b+1; b.resize(nTop); b=b-1 \n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:20:17.043862 Read test datas from pickle\n",
      "2016-06-05 16:20:25.799403 Read&Work train datas \n",
      "2016-06-05 16:20:38.641076 chunk : 0 (5000000, 12)\n",
      "2016-06-05 16:20:40.817127 after : 0 (4619903, 12)\n",
      "2016-06-05 16:20:40.817127 1\n",
      "2016-06-05 16:20:45.613283 grp=(train group) (1720455, 5)\n",
      "2016-06-05 16:20:45.613283 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 16:20:52.147468 grp=(new groups) 215750\n",
      "2016-06-05 16:20:55.086864 iGroup = 0 nn = 33 len(group) =  (1, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3-64\\lib\\site-packages\\ipykernel\\__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:23:48.299615 iGroup = 50000 nn = 324158 len(group) =  (1, 6)\n",
      "2016-06-05 16:27:14.263733 iGroup = 100000 nn = 632194 len(group) =  (1, 6)\n",
      "2016-06-05 16:30:38.757997 iGroup = 150000 nn = 961141 len(group) =  (1, 6)\n",
      "2016-06-05 16:34:02.796195 iGroup = 200000 nn = 1270241 len(group) =  (1, 6)\n",
      "2016-06-05 16:35:08.132112 2\n",
      "2016-06-05 16:35:10.163136 grp=(train group) (165192, 6)\n",
      "2016-06-05 16:35:10.163136 grpx=(test group) (43831, 4)\n",
      "2016-06-05 16:35:10.241260 grp=(new groups) 23475\n",
      "2016-06-05 16:35:10.970249 iGroup = 0 nn = 1 len(group) =  (19, 7)\n",
      "2016-06-05 16:36:48.747924 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3-64\\lib\\site-packages\\pandas\\core\\indexing.py:545: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:36:50.145873 grp=(train group) (187081, 4)\n",
      "2016-06-05 16:36:50.145873 grpx=(test group) (40718, 2)\n",
      "2016-06-05 16:36:50.192756 grp=(new groups) 24023\n",
      "2016-06-05 16:36:50.352688 iGroup = 0 nn = 1 len(group) =  (22, 5)\n",
      "2016-06-05 16:38:31.057831 3b\n",
      "2016-06-05 16:38:32.989127 grp=(train group) (675852, 5)\n",
      "2016-06-05 16:38:32.989127 grpx=(test group) (187210, 3)\n",
      "2016-06-05 16:38:33.197657 grp=(new groups) 91305\n",
      "2016-06-05 16:38:35.446800 iGroup = 0 nn = 1 len(group) =  (3, 6)\n",
      "2016-06-05 16:42:04.251515 iGroup = 50000 nn = 92203 len(group) =  (3, 6)\n",
      "2016-06-05 16:44:54.713918 4\n",
      "2016-06-05 16:44:55.778783 grp=(train group) (6381, 4)\n",
      "2016-06-05 16:44:55.778783 grpx=(test group) (206, 2)\n",
      "2016-06-05 16:44:55.784785 grp=(new groups) 203\n",
      "2016-06-05 16:44:55.791792 iGroup = 0 nn = 0 len(group) =  (42, 5)\n",
      "2016-06-05 16:45:09.255650 chunk : 1 (5000000, 12)\n",
      "2016-06-05 16:45:10.863567 after : 1 (4629399, 12)\n",
      "2016-06-05 16:45:10.863567 1\n",
      "2016-06-05 16:45:15.160729 grp=(train group) (1704133, 5)\n",
      "2016-06-05 16:45:15.160729 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 16:45:22.370494 grp=(new groups) 215884\n",
      "2016-06-05 16:45:25.795815 iGroup = 0 nn = 8 len(group) =  (1, 6)\n",
      "2016-06-05 16:48:55.984834 iGroup = 50000 nn = 331497 len(group) =  (1, 6)\n",
      "2016-06-05 16:52:22.106024 iGroup = 100000 nn = 631895 len(group) =  (1, 6)\n",
      "2016-06-05 16:55:46.293316 iGroup = 150000 nn = 962439 len(group) =  (1, 6)\n",
      "2016-06-05 16:59:45.063881 iGroup = 200000 nn = 1268414 len(group) =  (1, 6)\n",
      "2016-06-05 17:01:05.809128 2\n",
      "2016-06-05 17:01:07.758065 grp=(train group) (161951, 6)\n",
      "2016-06-05 17:01:07.758065 grpx=(test group) (43831, 4)\n",
      "2016-06-05 17:01:07.833717 grp=(new groups) 23129\n",
      "2016-06-05 17:01:08.561460 iGroup = 0 nn = 0 len(group) =  (2, 7)\n",
      "2016-06-05 17:02:44.074941 3\n",
      "2016-06-05 17:02:45.407327 grp=(train group) (183969, 4)\n",
      "2016-06-05 17:02:45.407327 grpx=(test group) (40718, 2)\n",
      "2016-06-05 17:02:45.460037 grp=(new groups) 23589\n",
      "2016-06-05 17:02:45.600679 iGroup = 0 nn = 0 len(group) =  (2, 5)\n",
      "2016-06-05 17:04:23.004516 3b\n",
      "2016-06-05 17:04:24.832255 grp=(train group) (665081, 5)\n",
      "2016-06-05 17:04:24.832255 grpx=(test group) (187210, 3)\n",
      "2016-06-05 17:04:25.060267 grp=(new groups) 90252\n",
      "2016-06-05 17:04:27.271482 iGroup = 0 nn = 0 len(group) =  (1, 6)\n",
      "2016-06-05 17:07:54.324501 iGroup = 50000 nn = 92885 len(group) =  (7, 6)\n",
      "2016-06-05 17:10:38.970751 4\n",
      "2016-06-05 17:10:40.013137 grp=(train group) (6390, 4)\n",
      "2016-06-05 17:10:40.013137 grpx=(test group) (206, 2)\n",
      "2016-06-05 17:10:40.013137 grp=(new groups) 203\n",
      "2016-06-05 17:10:40.028763 iGroup = 0 nn = 0 len(group) =  (43, 5)\n",
      "2016-06-05 17:10:53.254067 chunk : 2 (5000000, 12)\n",
      "2016-06-05 17:10:55.024179 after : 2 (4629129, 12)\n",
      "2016-06-05 17:10:55.039808 1\n",
      "2016-06-05 17:10:59.304374 grp=(train group) (1731332, 5)\n",
      "2016-06-05 17:10:59.304374 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 17:11:06.192196 grp=(new groups) 214522\n",
      "2016-06-05 17:11:09.570994 iGroup = 0 nn = 0 len(group) =  (1, 6)\n",
      "2016-06-05 17:14:34.592155 iGroup = 50000 nn = 328007 len(group) =  (1, 6)\n",
      "2016-06-05 17:18:05.315254 iGroup = 100000 nn = 635266 len(group) =  (2, 6)\n",
      "2016-06-05 17:21:28.247479 iGroup = 150000 nn = 977158 len(group) =  (1, 6)\n",
      "2016-06-05 17:24:50.337552 iGroup = 200000 nn = 1277758 len(group) =  (1, 6)\n",
      "2016-06-05 17:25:50.038202 2\n",
      "2016-06-05 17:25:51.980824 grp=(train group) (167029, 6)\n",
      "2016-06-05 17:25:51.980824 grpx=(test group) (43831, 4)\n",
      "2016-06-05 17:25:52.043332 grp=(new groups) 23820\n",
      "2016-06-05 17:25:52.764947 iGroup = 0 nn = 1 len(group) =  (24, 7)\n",
      "2016-06-05 17:27:37.346629 3\n",
      "2016-06-05 17:27:38.679589 grp=(train group) (188614, 4)\n",
      "2016-06-05 17:27:38.679589 grpx=(test group) (40718, 2)\n",
      "2016-06-05 17:27:38.723131 grp=(new groups) 24214\n",
      "2016-06-05 17:27:38.863760 iGroup = 0 nn = 1 len(group) =  (25, 5)\n",
      "2016-06-05 17:29:18.630873 3b\n",
      "2016-06-05 17:29:20.703755 grp=(train group) (683806, 5)\n",
      "2016-06-05 17:29:20.703755 grpx=(test group) (187210, 3)\n",
      "2016-06-05 17:29:20.933452 grp=(new groups) 92639\n",
      "2016-06-05 17:29:26.974528 iGroup = 0 nn = 1 len(group) =  (7, 6)\n",
      "2016-06-05 17:33:05.596966 iGroup = 50000 nn = 90983 len(group) =  (4, 6)\n",
      "2016-06-05 17:36:25.759117 4\n",
      "2016-06-05 17:36:28.422864 grp=(train group) (6387, 4)\n",
      "2016-06-05 17:36:28.422864 grpx=(test group) (206, 2)\n",
      "2016-06-05 17:36:28.438486 grp=(new groups) 201\n",
      "2016-06-05 17:36:28.454113 iGroup = 0 nn = 0 len(group) =  (42, 5)\n",
      "2016-06-05 17:36:43.489926 chunk : 3 (5000000, 12)\n",
      "2016-06-05 17:36:45.241958 after : 3 (4646171, 12)\n",
      "2016-06-05 17:36:45.241958 1\n",
      "2016-06-05 17:36:49.791397 grp=(train group) (1732315, 5)\n",
      "2016-06-05 17:36:49.791397 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 17:36:56.695028 grp=(new groups) 218353\n",
      "2016-06-05 17:37:02.501657 iGroup = 0 nn = 5 len(group) =  (1, 6)\n",
      "2016-06-05 17:40:41.930683 iGroup = 50000 nn = 325536 len(group) =  (1, 6)\n",
      "2016-06-05 17:44:05.772240 iGroup = 100000 nn = 629606 len(group) =  (1, 6)\n",
      "2016-06-05 17:47:36.543982 iGroup = 150000 nn = 948877 len(group) =  (1, 6)\n",
      "2016-06-05 17:50:58.930872 iGroup = 200000 nn = 1252811 len(group) =  (1, 6)\n",
      "2016-06-05 17:52:14.355587 2\n",
      "2016-06-05 17:52:16.358498 grp=(train group) (163520, 6)\n",
      "2016-06-05 17:52:16.358498 grpx=(test group) (43831, 4)\n",
      "2016-06-05 17:52:16.421008 grp=(new groups) 23221\n",
      "2016-06-05 17:52:17.145109 iGroup = 0 nn = 0 len(group) =  (1, 7)\n",
      "2016-06-05 17:53:52.607393 3\n",
      "2016-06-05 17:53:53.917435 grp=(train group) (184708, 4)\n",
      "2016-06-05 17:53:53.917435 grpx=(test group) (40718, 2)\n",
      "2016-06-05 17:53:53.979943 grp=(new groups) 23666\n",
      "2016-06-05 17:53:54.120576 iGroup = 0 nn = 0 len(group) =  (1, 5)\n",
      "2016-06-05 17:55:31.617576 3b\n",
      "2016-06-05 17:55:33.447868 grp=(train group) (669726, 5)\n",
      "2016-06-05 17:55:33.447868 grpx=(test group) (187210, 3)\n",
      "2016-06-05 17:55:33.682634 grp=(new groups) 90646\n",
      "2016-06-05 17:55:35.856001 iGroup = 0 nn = 1 len(group) =  (2, 6)\n",
      "2016-06-05 17:59:14.540958 iGroup = 50000 nn = 92779 len(group) =  (7, 6)\n",
      "2016-06-05 18:02:00.565146 4\n",
      "2016-06-05 18:02:01.621845 grp=(train group) (6359, 4)\n",
      "2016-06-05 18:02:01.621845 grpx=(test group) (206, 2)\n",
      "2016-06-05 18:02:01.637472 grp=(new groups) 202\n",
      "2016-06-05 18:02:01.637472 iGroup = 0 nn = 0 len(group) =  (43, 5)\n",
      "2016-06-05 18:02:14.958548 chunk : 4 (5000000, 12)\n",
      "2016-06-05 18:02:18.654420 after : 4 (4623948, 12)\n",
      "2016-06-05 18:02:18.654420 1\n",
      "2016-06-05 18:02:26.220227 grp=(train group) (1717221, 5)\n",
      "2016-06-05 18:02:26.220227 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 18:02:34.680832 grp=(new groups) 215505\n",
      "2016-06-05 18:02:37.954271 iGroup = 0 nn = 9 len(group) =  (1, 6)\n",
      "2016-06-05 18:06:14.635893 iGroup = 50000 nn = 330294 len(group) =  (1, 6)\n",
      "2016-06-05 18:11:24.257029 iGroup = 100000 nn = 633438 len(group) =  (1, 6)\n",
      "2016-06-05 18:15:02.740085 iGroup = 150000 nn = 966463 len(group) =  (1, 6)\n",
      "2016-06-05 18:24:28.493085 iGroup = 200000 nn = 1274521 len(group) =  (1, 6)\n",
      "2016-06-05 18:27:38.145290 2\n",
      "2016-06-05 18:27:43.572017 grp=(train group) (164665, 6)\n",
      "2016-06-05 18:27:43.572017 grpx=(test group) (43831, 4)\n",
      "2016-06-05 18:27:43.681395 grp=(new groups) 23479\n",
      "2016-06-05 18:27:44.985195 iGroup = 0 nn = 1 len(group) =  (23, 7)\n",
      "2016-06-05 18:32:34.349554 3\n",
      "2016-06-05 18:32:39.651381 grp=(train group) (186450, 4)\n",
      "2016-06-05 18:32:39.651381 grpx=(test group) (40718, 2)\n",
      "2016-06-05 18:32:39.854515 grp=(new groups) 23856\n",
      "2016-06-05 18:32:40.639787 iGroup = 0 nn = 1 len(group) =  (23, 5)\n",
      "2016-06-05 18:37:54.026362 3b\n",
      "2016-06-05 18:38:00.987711 grp=(train group) (675478, 5)\n",
      "2016-06-05 18:38:00.987711 grpx=(test group) (187210, 3)\n",
      "2016-06-05 18:38:01.794581 grp=(new groups) 91759\n",
      "2016-06-05 18:38:08.728539 iGroup = 0 nn = 1 len(group) =  (14, 6)\n",
      "2016-06-05 18:48:53.036290 iGroup = 50000 nn = 91651 len(group) =  (6, 6)\n",
      "2016-06-05 18:52:19.702162 4\n",
      "2016-06-05 18:52:20.738429 grp=(train group) (6390, 4)\n",
      "2016-06-05 18:52:20.738429 grpx=(test group) (206, 2)\n",
      "2016-06-05 18:52:20.754054 grp=(new groups) 203\n",
      "2016-06-05 18:52:20.754054 iGroup = 0 nn = 0 len(group) =  (44, 5)\n",
      "2016-06-05 18:52:34.828868 chunk : 5 (5000000, 12)\n",
      "2016-06-05 18:52:36.609688 after : 5 (4629094, 12)\n",
      "2016-06-05 18:52:36.609688 1\n",
      "2016-06-05 18:52:47.311921 grp=(train group) (1730131, 5)\n",
      "2016-06-05 18:52:47.311921 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 18:52:56.124813 grp=(new groups) 216840\n",
      "2016-06-05 18:52:59.547897 iGroup = 0 nn = 34 len(group) =  (1, 6)\n",
      "2016-06-05 18:56:22.588464 iGroup = 50000 nn = 324976 len(group) =  (1, 6)\n",
      "2016-06-05 18:59:55.884449 iGroup = 100000 nn = 631872 len(group) =  (1, 6)\n",
      "2016-06-05 19:03:50.103878 iGroup = 150000 nn = 954662 len(group) =  (1, 6)\n",
      "2016-06-05 19:10:40.732205 iGroup = 200000 nn = 1260996 len(group) =  (1, 6)\n",
      "2016-06-05 19:14:29.872624 2\n",
      "2016-06-05 19:14:33.121546 grp=(train group) (166041, 6)\n",
      "2016-06-05 19:14:33.121546 grpx=(test group) (43831, 4)\n",
      "2016-06-05 19:14:33.246550 grp=(new groups) 23686\n",
      "2016-06-05 19:14:34.649266 iGroup = 0 nn = 0 len(group) =  (2, 7)\n",
      "2016-06-05 19:16:16.943747 3\n",
      "2016-06-05 19:16:18.295661 grp=(train group) (188358, 4)\n",
      "2016-06-05 19:16:18.295661 grpx=(test group) (40718, 2)\n",
      "2016-06-05 19:16:18.342548 grp=(new groups) 24082\n",
      "2016-06-05 19:16:18.498801 iGroup = 0 nn = 0 len(group) =  (2, 5)\n",
      "2016-06-05 19:17:58.500683 3b\n",
      "2016-06-05 19:18:00.435197 grp=(train group) (679394, 5)\n",
      "2016-06-05 19:18:00.435197 grpx=(test group) (187210, 3)\n",
      "2016-06-05 19:18:00.665957 grp=(new groups) 92105\n",
      "2016-06-05 19:18:02.894997 iGroup = 0 nn = 0 len(group) =  (1, 6)\n",
      "2016-06-05 19:21:30.601167 iGroup = 50000 nn = 91360 len(group) =  (1, 6)\n",
      "2016-06-05 19:24:21.732124 4\n",
      "2016-06-05 19:24:22.793562 grp=(train group) (6379, 4)\n",
      "2016-06-05 19:24:22.793562 grpx=(test group) (206, 2)\n",
      "2016-06-05 19:24:22.809179 grp=(new groups) 203\n",
      "2016-06-05 19:24:22.809179 iGroup = 0 nn = 0 len(group) =  (43, 5)\n",
      "2016-06-05 19:24:36.972667 chunk : 6 (5000000, 12)\n",
      "2016-06-05 19:24:38.781038 after : 6 (4643769, 12)\n",
      "2016-06-05 19:24:38.781038 1\n",
      "2016-06-05 19:24:43.203266 grp=(train group) (1734752, 5)\n",
      "2016-06-05 19:24:43.203266 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 19:24:50.773723 grp=(new groups) 218228\n",
      "2016-06-05 19:24:54.413104 iGroup = 0 nn = 22 len(group) =  (1, 6)\n",
      "2016-06-05 19:28:27.608356 iGroup = 50000 nn = 328049 len(group) =  (1, 6)\n",
      "2016-06-05 19:31:51.024047 iGroup = 100000 nn = 629930 len(group) =  (1, 6)\n",
      "2016-06-05 19:36:55.061593 iGroup = 150000 nn = 952897 len(group) =  (1, 6)\n",
      "2016-06-05 19:40:16.584531 iGroup = 200000 nn = 1256730 len(group) =  (2, 6)\n",
      "2016-06-05 19:41:31.040544 2\n",
      "2016-06-05 19:41:32.889911 grp=(train group) (162361, 6)\n",
      "2016-06-05 19:41:32.889911 grpx=(test group) (43831, 4)\n",
      "2016-06-05 19:41:32.968029 grp=(new groups) 23150\n",
      "2016-06-05 19:41:33.673618 iGroup = 0 nn = 1 len(group) =  (20, 7)\n",
      "2016-06-05 19:43:12.144691 3\n",
      "2016-06-05 19:43:13.480350 grp=(train group) (185181, 4)\n",
      "2016-06-05 19:43:13.480350 grpx=(test group) (40718, 2)\n",
      "2016-06-05 19:43:13.542863 grp=(new groups) 23730\n",
      "2016-06-05 19:43:13.683487 iGroup = 0 nn = 1 len(group) =  (22, 5)\n",
      "2016-06-05 19:44:56.105796 3b\n",
      "2016-06-05 19:44:57.945405 grp=(train group) (668406, 5)\n",
      "2016-06-05 19:44:57.945405 grpx=(test group) (187210, 3)\n",
      "2016-06-05 19:44:58.165183 grp=(new groups) 90488\n",
      "2016-06-05 19:45:00.539186 iGroup = 0 nn = 1 len(group) =  (1, 6)\n",
      "2016-06-05 19:49:43.507244 iGroup = 50000 nn = 92755 len(group) =  (4, 6)\n",
      "2016-06-05 19:53:38.299830 4\n",
      "2016-06-05 19:53:39.325245 grp=(train group) (6366, 4)\n",
      "2016-06-05 19:53:39.325245 grpx=(test group) (206, 2)\n",
      "2016-06-05 19:53:39.325245 grp=(new groups) 204\n",
      "2016-06-05 19:53:39.340863 iGroup = 0 nn = 0 len(group) =  (44, 5)\n",
      "2016-06-05 19:53:46.723451 chunk : 7 (2670293, 12)\n",
      "2016-06-05 19:53:47.679707 after : 7 (2476425, 12)\n",
      "2016-06-05 19:53:47.679707 1\n",
      "2016-06-05 19:53:49.845748 grp=(train group) (977402, 5)\n",
      "2016-06-05 19:53:49.845748 grpx=(test group) (1385522, 3)\n",
      "2016-06-05 19:53:54.735301 grp=(new groups) 144265\n",
      "2016-06-05 19:53:57.127592 iGroup = 0 nn = 115 len(group) =  (1, 6)\n",
      "2016-06-05 19:59:02.109182 iGroup = 50000 nn = 502022 len(group) =  (1, 6)\n",
      "2016-06-05 20:02:26.845224 iGroup = 100000 nn = 950735 len(group) =  (1, 6)\n",
      "2016-06-05 20:05:34.475667 2\n",
      "2016-06-05 20:05:35.573636 grp=(train group) (127822, 6)\n",
      "2016-06-05 20:05:35.573636 grpx=(test group) (43831, 4)\n",
      "2016-06-05 20:05:35.636139 grp=(new groups) 19650\n",
      "2016-06-05 20:05:36.266559 iGroup = 0 nn = 1 len(group) =  (16, 7)\n",
      "2016-06-05 20:07:20.784114 3\n",
      "2016-06-05 20:07:22.475172 grp=(train group) (146443, 4)\n",
      "2016-06-05 20:07:22.475172 grpx=(test group) (40718, 2)\n",
      "2016-06-05 20:07:22.553303 grp=(new groups) 20325\n",
      "2016-06-05 20:07:22.769121 iGroup = 0 nn = 1 len(group) =  (17, 5)\n",
      "2016-06-05 20:11:16.775132 3b\n",
      "2016-06-05 20:11:18.938617 grp=(train group) (482976, 5)\n",
      "2016-06-05 20:11:18.938617 grpx=(test group) (187210, 3)\n",
      "2016-06-05 20:11:19.222988 grp=(new groups) 73740\n",
      "2016-06-05 20:11:25.583658 iGroup = 0 nn = 2 len(group) =  (2, 6)\n",
      "2016-06-05 20:22:54.028729 iGroup = 50000 nn = 108589 len(group) =  (1, 6)\n",
      "2016-06-05 20:27:44.260674 4\n",
      "2016-06-05 20:27:45.530955 grp=(train group) (6103, 4)\n",
      "2016-06-05 20:27:45.530955 grpx=(test group) (206, 2)\n",
      "2016-06-05 20:27:45.546584 grp=(new groups) 199\n",
      "2016-06-05 20:27:45.546584 iGroup = 0 nn = 0 len(group) =  (40, 5)\n",
      "2016-06-05 20:27:59.878942 Build array5 begin\n",
      "(1385522, 5)\n",
      "(43831, 5)\n",
      "(40718, 5)\n",
      "(187210, 5)\n",
      "(206, 5)\n",
      "2016-06-05 21:57:32.605631 Build array5 end\n",
      "2016-06-05 21:57:34.929223 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for train data (chunks) if not in files\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(datetime.datetime.now(),'Read test datas from pickle')\n",
    "\n",
    "rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "rule3bdf= pd.read_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'))\n",
    "rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "rule1Test = [rule1df,rule1ar]\n",
    "rule2Test = [rule2df,rule2ar]\n",
    "rule3Test = [rule3df,rule3ar]\n",
    "rule3bTest= [rule3bdf,rule3bar]\n",
    "rule4Test = [rule4df,rule4ar]\n",
    "del rule1df,rule2df,rule3df,rule3bdf,rule4df\n",
    "del rule1ar,rule2ar,rule3ar,rule3bar,rule4ar\n",
    "# ------------------------------------------------------------------------------    \n",
    "print(datetime.datetime.now(),'Read&Work train datas ')\n",
    "chunkSize = 5000000\n",
    "\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3bTest1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "\n",
    "if not OK :        \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=ruleTrain+['user_id']);\n",
    "    readerTrain\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (i>1) : print(datetime.datetime.now(),'break :',i); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # Estimate influence for other user groups\n",
    "        if True :\n",
    "            chunk   = chunk.merge(grps20l,how='inner',on=['user_id'])\n",
    "            print(datetime.datetime.now(),'after :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'1'); iGroup = 0;\n",
    "        grp = chunk[rule1+ruleXtrain].groupby(rule1+['hotel_cluster']).sum().reset_index()\n",
    "        grp.loc[:,'cnt'] = 1 # grp.cnt*(3+17*grp.is_booking)\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule1Test[0].shape)\n",
    "        grp = grp.merge(rule1Test[0],how='inner',on=rule1,copy=False).groupby(rule1)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule1Test[1])\n",
    "        #ruleWork(grp,rule1Test)\n",
    "        \n",
    "\n",
    "        # Rule2 --> (year==2014) : srch_destination_id+hotel_country+hotel_market += app1\n",
    "        print(datetime.datetime.now(),'2'); iGroup = 0;\n",
    "        grp = chunk.query('dty==2014')[rule2+ruleXtrain];\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule2+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule2Test[0].shape)\n",
    "        grp = grp.merge(rule2Test[0],how='inner',on=rule2,copy=False).groupby(rule2)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule2Test[1])\n",
    "        #ruleWork(grp,rule2Test,b0Add=3,b1Add=20)\n",
    "        \n",
    "        # Rule3 --> srch_destination_id += app1\n",
    "        print(datetime.datetime.now(),'3'); iGroup = 0;\n",
    "        grp = chunk[rule3+ruleXtrain]\n",
    "        #grp.loc[:,'cnt'] = grp.cnt*(3+17*grp.is_booking)\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule3+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule3Test[0].shape)\n",
    "        grp = grp.merge(rule3Test[0],how='inner',on=rule3).groupby(rule3)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule3Test[1])\n",
    "        \n",
    "        # Rule3b --> srch_destination_id+dt0m += app1\n",
    "        print(datetime.datetime.now(),'3b'); iGroup = 0;\n",
    "        grp = chunk[rule3b+ruleXtrain]\n",
    "        #grp.loc[:,'cnt'] = grp.cnt*(3+17*grp.is_booking)\n",
    "        #grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        # cnt=cnt \n",
    "        grp = grp.groupby(rule3b+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule3bTest[0].shape)\n",
    "        grp = grp.merge(rule3bTest[0],how='inner',on=rule3b).groupby(rule3b)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule3bTest[1])\n",
    "        \n",
    "        # Rule4 --> hotel_country += app2\n",
    "        print(datetime.datetime.now(),'4'); iGroup = 0;\n",
    "        grp = chunk[rule4+ruleXtrain]\n",
    "        grp.loc[:,'cnt'] = (1+5*grp.is_booking) # *grp.cnt\n",
    "        grp = grp.groupby(rule4+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule4Test[0].shape)\n",
    "        grp = grp.merge(rule4Test[0],how='inner',on=rule4).groupby(rule4)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule4Test[1])\n",
    "        \n",
    "    del readerTrain\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1.pkl.npy'),rule3Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3bTest1.pkl.npy'),rule3bTest[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "    print(datetime.datetime.now(),'Build array5 begin')\n",
    "    rule1Arr5 = np.apply_along_axis(findTop,1,rule1Test[1]); print(rule1Arr5.shape)\n",
    "    rule2Arr5 = np.apply_along_axis(findTop,1,rule2Test[1]); print(rule2Arr5.shape)\n",
    "    rule3Arr5 = np.apply_along_axis(findTop,1,rule3Test[1]); print(rule3Arr5.shape)\n",
    "    rule3bArr5= np.apply_along_axis(findTop,1,rule3bTest[1]); print(rule3bArr5.shape)\n",
    "    rule4Arr5 = np.apply_along_axis(findTop,1,rule4Test[1]); print(rule4Arr5.shape)\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test2.pkl.npy'),rule1Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule2Test2.pkl.npy'),rule2Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule3Test2.pkl.npy'),rule3Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule3bTest2.pkl.npy'),rule3bArr5)\n",
    "    np.save(os.path.join(dirTemp,'rule4Test2.pkl.npy'),rule4Arr5)\n",
    "\n",
    "    print(datetime.datetime.now(),'Build array5 end')\n",
    "    \n",
    "else :  \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1.pkl.npy'))\n",
    "    rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "    \n",
    "    rule1Test[1] = rule1ar\n",
    "    rule2Test[1] = rule2ar\n",
    "    rule3Test[1] = rule3ar\n",
    "    rule3bTest[1] = rule3bar\n",
    "    rule4Test[1] = rule4ar\n",
    "    del rule1ar,rule2ar,rule3ar,rule3bar,rule4ar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "    \n",
    "    rule1Arr5 = np.load(os.path.join(dirTemp,'rule1Test2.pkl.npy'))\n",
    "    rule2Arr5 = np.load(os.path.join(dirTemp,'rule2Test2.pkl.npy'))\n",
    "    rule3Arr5 = np.load(os.path.join(dirTemp,'rule3Test2.pkl.npy'))\n",
    "    rule3bArr5= np.load(os.path.join(dirTemp,'rule3bTest2.pkl.npy'))\n",
    "    rule4Arr5 = np.load(os.path.join(dirTemp,'rule4Test2.pkl.npy'))\n",
    "    \n",
    "# Top clusters\n",
    "rule5Arr5 = np.array([91,42,59,28])\n",
    "        \n",
    "print(datetime.datetime.now(),'Done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 04:10:49.676898 User stats \n",
      "2016-06-05 04:11:18.640001 chunk : 0 (10000000, 8)\n",
      "2016-06-05 04:11:18.640001 11\n",
      "2016-06-05 04:12:19.153787 chunk : 1 (10000000, 8)\n",
      "2016-06-05 04:12:19.153787 11\n",
      "2016-06-05 04:12:55.648885 chunk : 2 (10000000, 8)\n",
      "2016-06-05 04:12:55.648885 11\n",
      "2016-06-05 04:13:17.935272 chunk : 3 (7670293, 8)\n",
      "2016-06-05 04:13:17.935272 11\n",
      "(3000693, 8)\n",
      "2016-06-05 04:13:18.452991 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate EHR rules Rule1\n",
    "if False :\n",
    "    \n",
    "    print(datetime.datetime.now(),'EHR rules ')\n",
    "\n",
    "    chunkSize   = 10000000\n",
    "    rule11      = ['user_id','user_location_city','srch_destination_id','hotel_market','hotel_country']\n",
    "    rule11Xtrain= rule11 + ruleXtrain\n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=rule11Xtrain);\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'11'); iGroup = 0;\n",
    "        \n",
    "        grp = chunk[(chunk.is_booking>=1) & chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & chunk.hotel_cluster.notnull()]\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    #grps = grps.groupby('user_id').sum().reset_index()\n",
    "    print(grps.shape)    \n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2578334"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps.head()\n",
    "grpss = grps.groupby(['user_id','user_location_city','srch_destination_id','hotel_market','hotel_country'])\n",
    "len(grpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1385522, 100)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule1Test[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate EHR rules Rule2\n",
    "if False :\n",
    "    \n",
    "    print(datetime.datetime.now(),'EHR rules ')\n",
    "\n",
    "    chunkSize   = 10000000\n",
    "    rule12      = ['user_id','srch_destination_id','hotel_market','hotel_country']\n",
    "    rule12Xtrain= rule12 + ruleXtrain\n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=rule11Xtrain);\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'11'); iGroup = 0;\n",
    "        \n",
    "        grp = chunk[(chunk.is_booking>=1) & chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & chunk.hotel_cluster.notnull()]\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    #grps = grps.groupby('user_id').sum().reset_index()\n",
    "    print(grps.shape)    \n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:15:45.070508 User stats \n",
      "2016-06-05 16:16:06.045301 chunk : 0 (10000000, 5)\n",
      "2016-06-05 16:16:06.045301 1\n",
      "2016-06-05 16:16:32.635124 chunk : 1 (10000000, 5)\n",
      "2016-06-05 16:16:32.635124 1\n",
      "2016-06-05 16:17:12.123193 chunk : 2 (10000000, 5)\n",
      "2016-06-05 16:17:12.123193 1\n",
      "2016-06-05 16:17:33.376702 chunk : 3 (7670293, 5)\n",
      "2016-06-05 16:17:33.376702 1\n",
      "(1198786, 3)\n",
      "2016-06-05 16:17:34.751060 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate user_id stats\n",
    "if True :\n",
    "    \n",
    "    print(datetime.datetime.now(),'User stats ')\n",
    "\n",
    "    chunkSize = 10000000\n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=['user_id','is_booking','cnt','dt0y','dt0m']);\n",
    "    readerTrain\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'1'); iGroup = 0;\n",
    "        grp = chunk[['user_id','is_booking','cnt']].groupby('user_id').sum().reset_index()\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    grps = grps.groupby('user_id').sum().reset_index()\n",
    "    print(grps.shape)    \n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198786"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps.groupby('is_booking').count().reset_index().head(10)\n",
    "len(grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec809bd828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps[grps.is_booking<20].is_booking.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184100, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps20l = grps[grps.is_booking<=20][['user_id']]; grps20l.head()\n",
    "grps20g = grps[grps.is_booking>20][['user_id']]; \n",
    "grps20l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rxEstimate (rx,hcl) :\n",
    "    rxx = np.zeros(rx.shape[1],dtype=int)\n",
    "    for i in range(rx.shape[1]) :\n",
    "        xx = (hcl==pd.Series(rx[:,i])) ; xx=xx.groupby(by=xx).count(); \n",
    "        try     : rxx[i] = xx[True]\n",
    "        except  : rxx[i] = 0 \n",
    "    return(rxx)\n",
    "def estimate (rxx) : \n",
    "    res = 0.0\n",
    "    for i in range(rxx.shape[0]) : res += rxx[i]/(i+1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-06 09:19:28.180212 Validation  debug= False\n",
      "2016-06-06 09:19:32.732125 chunk : 0 (1000000, 12)\n",
      "2016-06-06 09:21:35.608241 chunk : 1 (1000000, 12)\n",
      "2016-06-06 09:23:37.499446 chunk : 2 (1000000, 12)\n",
      "2016-06-06 09:25:35.613262 chunk : 3 (145036, 12)\n",
      "----> result =  1093247.0333328506 0.3476103400192718 3145036\n",
      "2016-06-06 09:25:49.221224 Done\n"
     ]
    }
   ],
   "source": [
    "# Validation on train(is_booking==1) datas\n",
    "Validation = True\n",
    "if Validation :        \n",
    "    debugV  = True\n",
    "    debugV  = False\n",
    "    \n",
    "    print(datetime.datetime.now(),'Validation',' debug=',debugV)\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    \n",
    "    chunkSizeV   = 1000000\n",
    "    if debugV : chunkSizeV   = 10000\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileValidation,chunksize=chunkSizeV,usecols=ruleValidate+['user_id']);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = 0.0; resultl = []\n",
    "    \n",
    "    r1xx   = np.zeros((nTop),dtype=int)\n",
    "    r2xx   = np.zeros((nTop),dtype=int)\n",
    "    r3xx   = np.zeros((nTop),dtype=int)\n",
    "    r4xx   = np.zeros((nTop),dtype=int)\n",
    "    r3bxx  = np.zeros((nTop),dtype=int)\n",
    "    \n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    \n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        if (ichunk>1) and debugV : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        # Estimate influence for other user groups\n",
    "        #chunk   = chunk.merge(grps20l,how='inner',on=['user_id'])\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_4\",\"_3b\"]); #print(chunk.head())\n",
    "        \n",
    "        chunk.rename(columns={'nn':'nn_3b'},inplace=True)\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); rr1=rr\n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); rr2=rr\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); rr3=rr\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); rr4=rr\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        #---------------------------- add rule 3a\n",
    "        \n",
    "        rr  = chunk['nn_3b'].copy(); rr[chunk.nn_3b.isnull()] = 0; rr=rr.astype(int); rr3b=rr\n",
    "        r3bx = rule3bArr5[rr.values];\n",
    "        r3bx[chunk.nn_3b.isnull().values] = rz;\n",
    "        \n",
    "        hcl = chunk['hotel_cluster']\n",
    "        \n",
    "        \n",
    "        r1xx += rxEstimate(r1x,hcl)\n",
    "        r2xx += rxEstimate(r2x,hcl)\n",
    "        r3xx += rxEstimate(r3x,hcl)\n",
    "        r4xx += rxEstimate(r4x,hcl)\n",
    "        r3bxx+= rxEstimate(r3bx,hcl)\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            ra, rb  = [],[]\n",
    "            nEvent += 1;\n",
    "            xEvent  = 0;\n",
    "            \n",
    "            rrr1 = rule1Test[1][rr1[irow]]\n",
    "            rrr2 = rule1Test[1][rr2[irow]]\n",
    "            \n",
    "            rr   = r1x[irow].tolist()+r2x[irow].tolist()+r3x[irow].tolist()+r4x[irow].tolist();\n",
    "            #rr   = r2x[irow].tolist()+r1x[irow].tolist()+r3x[irow].tolist()+r4x[irow].tolist();\n",
    "            res5 = [i for i in rr if i>-1][0:nTop]\n",
    "                \n",
    "            res = 0.0\n",
    "            if hcl[irow] in res5 : res = 1/(res5.index(hcl[irow])+1)\n",
    "            result += res\n",
    "            resultl.append([res,xEvent])\n",
    "            \n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "            \n",
    "        if (ichunk>=1) and debugV : print(datetime.datetime.now(),'break'); break\n",
    "        \n",
    "    print('----> result = ',result,result/nEvent,len(resultl))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dty</th>\n",
       "      <th>user_location_city</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <th>user_id</th>\n",
       "      <th>dt0y</th>\n",
       "      <th>dt0m</th>\n",
       "      <th>srch_destination_id</th>\n",
       "      <th>is_booking</th>\n",
       "      <th>cnt</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>hotel_market</th>\n",
       "      <th>hotel_cluster</th>\n",
       "      <th>nn_1</th>\n",
       "      <th>nn_2</th>\n",
       "      <th>nn_3</th>\n",
       "      <th>nn_4</th>\n",
       "      <th>nn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>35390</td>\n",
       "      <td>911.5142</td>\n",
       "      <td>93</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>14984</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1457</td>\n",
       "      <td>92</td>\n",
       "      <td>866270.0</td>\n",
       "      <td>11511.0</td>\n",
       "      <td>10486.0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>10067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>501</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>8267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>675</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6058.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>69</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6494.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>1236</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6497.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>1197</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6496.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dty  user_location_city  orig_destination_distance  user_id  dt0y  dt0m  \\\n",
       "0  2014               35390                   911.5142       93  2014    11   \n",
       "1  2014               10067                        NaN      501  2014     8   \n",
       "2  2014               47725                        NaN     1048  2015     6   \n",
       "3  2014               47725                        NaN     1048  2015     6   \n",
       "4  2014               47725                        NaN     1048  2015     6   \n",
       "\n",
       "   srch_destination_id  is_booking  cnt  hotel_country  hotel_market  \\\n",
       "0                14984           0    1             50          1457   \n",
       "1                 8267           0    1             50           675   \n",
       "2                 8803           0    1            151            69   \n",
       "3                 8803           0    1            151          1236   \n",
       "4                 8803           0    1            151          1197   \n",
       "\n",
       "   hotel_cluster      nn_1     nn_2     nn_3  nn_4   nn  \n",
       "0             92  866270.0  11511.0  10486.0    47   47  \n",
       "1             98       NaN   6058.0   5697.0    47   47  \n",
       "2             85       NaN   6494.0   6063.0   145  145  \n",
       "3             82       NaN   6497.0   6063.0   145  145  \n",
       "4             29       NaN   6496.0   6063.0   145  145  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[449755  44174   3343    223     16] 497511 [  9.04010163e-01   8.87899966e-02   6.71944942e-03   4.48231295e-04\n",
      "   3.21600929e-05] \n",
      " [423413 304371 234696 191717 164568] 1318765 [ 0.32106782  0.23080003  0.17796651  0.14537617  0.12478948] \n",
      " [390229 281437 228103 179475 158187] 1237431 \n",
      " [116970 102491 124696  87057  76522] 507736\n",
      "[45980 56797 50872 54399 64965] 273013 [ 0.16841689  0.20803771  0.18633545  0.19925425  0.2379557 ]\n",
      "0.150400594249\n",
      "0.23359775532\n",
      "0.217322785282\n",
      "0.0784886034161\n",
      "0.0374967355965\n"
     ]
    }
   ],
   "source": [
    "print(r1xx,r1xx.sum(),r1xx/r1xx.sum(),'\\n',r2xx,r2xx.sum(),r2xx/r2xx.sum(),'\\n',r3xx,r3xx.sum(),'\\n',r4xx,r4xx.sum())\n",
    "print(r3bxx,r3bxx.sum(),r3bxx/r3bxx.sum())\n",
    "print(estimate(r1xx)/nEvent)\n",
    "print(estimate(r2xx)/nEvent)\n",
    "print(estimate(r3xx)/nEvent)\n",
    "print(estimate(r4xx)/nEvent)\n",
    "print(estimate(r3bxx)/nEvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultl\n",
    "res=pd.DataFrame(resultl); res.columns=['result','who']; res['cumres']=res['result'].cumsum()\n",
    "#res['who'].hist()\n",
    "#res['result'].hist()\n",
    "#res['cumres'].plot()\n",
    "res[['cumres']].plot()\n",
    "#res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-06 09:45:00.190060 Build result\n",
      "2016-06-06 09:45:05.652017 chunk : 0 (1000000, 9)\n",
      "2016-06-06 09:46:57.170427 chunk : 1 (1000000, 9)\n",
      "2016-06-06 09:48:52.526702 chunk : 2 (528243, 9)\n",
      "----> result =  [0, 1, 2, 3, 4] ['5 37 55 22 11', '5 35 37 41 22', '91 31 96 77 48', '1 50 51 54 10', '50 51 91 2 59'] 2528243\n",
      "2016-06-06 09:50:43.501187 Done\n"
     ]
    }
   ],
   "source": [
    "Submission = True\n",
    "if Submission :        \n",
    "    print(datetime.datetime.now(),'Build result')\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=1000000,usecols=ruleTest);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = []\n",
    "    id     = []\n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    \n",
    "    r5x    = rule5Arr5\n",
    "    \n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        #if (ichunk>3) : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE?)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); \n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        id= id+chunk['id'].tolist()\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            nEvent += 1;\n",
    "            \n",
    "            res5 =        [i for i in r1x[irow] if (i>-1)]\n",
    "            res5 = res5 + [i for i in r2x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r3x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r4x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r5x       if (i>-1) and (i not in res5)]\n",
    "            \n",
    "            \n",
    "                \n",
    "            result.append(res5[0:nTop])\n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "        \n",
    "    result = [x.__str__().replace(',','')[1:-1] for x in result];\n",
    "    print('----> result = ',id[0:5],result[0:5],len(result))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\Result\\\\Leak\\\\df-Leak-2016-06-06-09-51-24.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testResult = pd.DataFrame({'id':id,'hotel_cluster':result})\n",
    "fileResult = os.path.join(dirResult,'df-Leak-'+datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.csv'); fileResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hotel_cluster  id\n",
      "0   5 37 55 22 11   0\n",
      "1   5 35 37 41 22   1\n",
      "2  91 31 96 77 48   2\n",
      "3   1 50 51 54 10   3\n",
      "4   50 51 91 2 59   4\n",
      "5  91 42 95 48 39   5\n",
      "6   95 21 98 91 2   6\n",
      "7  41 59 21 37 28   7\n",
      "8  88 50 51 54 10   8\n",
      "9  55 32 34 10 50   9           hotel_cluster       id\n",
      "2528233   21 59 95 4 19  2528233\n",
      "2528234  59 87 65 81 52  2528234\n",
      "2528235   82 85 36 7 78  2528235\n",
      "2528236   85 7 93 48 15  2528236\n",
      "2528237  71 80 84 92 90  2528237\n",
      "2528238   34 26 73 0 84  2528238\n",
      "2528239  57 62 46 36 82  2528239\n",
      "2528240  54 50 51 10 94  2528240\n",
      "2528241  50 47 43 15 32  2528241\n",
      "2528242  12 36 57 62 81  2528242\n"
     ]
    }
   ],
   "source": [
    "print(testResult.head(10),testResult.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   hotel_cluster\n",
      "0   0   5 37 55 22 11\n",
      "1   1   5 35 37 41 22\n",
      "2   2  91 31 96 77 48\n",
      "3   3   1 50 51 54 10\n",
      "4   4   50 51 91 2 59\n"
     ]
    }
   ],
   "source": [
    "print(testResult[['id','hotel_cluster']].head())\n",
    "testResult[['hotel_cluster']].to_csv(fileResult,header=True,index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1Test[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx=pd.DataFrame(rule1Arr5)\n",
    "xx['r0'] =rule1Test[0]['user_location_city']\n",
    "xx['r1'] =rule1Test[0]['orig_destination_distance']\n",
    "xx['r1x']=round(rule1Test[0]['orig_destination_distance'])\n",
    "xx.head()\n",
    "len(xx.groupby(['r0','r1x'])); len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx.columns\n",
    "xx[['r1',0]].query('r1>11000').plot(kind='scatter',x='r1',y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build round version ODD variable\n",
    "rule1Test0 = rule1Test[0].copy();\n",
    "rule1Test0['orig_destination_distance']=round(rule1Test0['orig_destination_distance'],-1);\n",
    "yy = rule1Test0.groupby(['user_location_city','orig_destination_distance']).count().reset_index(); yy.nn=yy.index; \n",
    "print(yy.shape,'\\n',yy.head())\n",
    "\n",
    "xx = rule1Test0.merge(yy,how='left',on=rule1,suffixes=['_l','_r']); print(len(xx))\n",
    "\n",
    "rule1Test0 = yy\n",
    "rule1Test1 = np.zeros((len(yy),nClusters),dtype=int); \n",
    "for i in range(len(xx)) : rule1Test1[xx.nn_r[i]] += rule1Test[1][xx.nn_l[i]]\n",
    "    \n",
    "print(rule1Test[1][0])\n",
    "print(rule1Test1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX = np.apply_along_axis(findTop,1,rule1Test1); print(rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change on round version\n",
    "rule1Test[0]=rule1Test0\n",
    "rule1Test[1]=rule1Test1\n",
    "rule1Arr5=rule1ArrX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print(rule1Test0.shape,rule1Test1.shape,rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX[0:10]; pd.Series(rule1ArrX[:,4]).groupby(rule1ArrX[:,0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rule1ArrX).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
