{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import datetime\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables: dirs and files\n",
    "\n",
    "dirBase   = '..'\n",
    "\n",
    "dirData   = os.path.join(dirBase,'Data')\n",
    "dirPicle  = os.path.join(dirBase,'Data','Pickle')\n",
    "#dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-1')\n",
    "#dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-2')\n",
    "dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-3')\n",
    "dirResult = os.path.join(dirBase,'Result','Leak') \n",
    "\n",
    "fileTrain      = os.path.join(dirData,'trainExt.csv')\n",
    "fileValidation = os.path.join(dirData,'trainExtMin.csv')\n",
    "fileTest       = os.path.join(dirData,'testExt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "chunkSize = 4000000\n",
    "nClusters = 100\n",
    "nTop      = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rules field\n",
    "\n",
    "rule1  = ['user_location_city','orig_destination_distance']\n",
    "rule2  = ['srch_destination_id','hotel_country','hotel_market']\n",
    "rule3  = ['srch_destination_id']\n",
    "rule3b = ['srch_destination_id','dt0m']\n",
    "rule4  = ['hotel_country']\n",
    "\n",
    "rule11 = ['user_id','user_location_city','srch_destination_id','hotel_market','hotel_country']\n",
    "rule12 = ['user_id','srch_destination_id','hotel_market','hotel_country']\n",
    "rule13 = ['user_id','srch_destination_id']\n",
    "\n",
    "ruleXtest    = ['cnt','is_booking']\n",
    "ruleXtrain   = ['hotel_cluster'] + ruleXtest\n",
    "\n",
    "\n",
    "ruleTrain    = rule1+rule2+[i for i in rule3b if i not in rule2]+[i for i in rule11 if i not in rule1+rule2]+ruleXtrain+['dt0y','dty']\n",
    "ruleTest     = rule1+rule2+[i for i in rule3b if i not in rule2]+[i for i in rule11 if i not in rule1+rule2]+ruleXtest+['id']\n",
    "ruleValidate = ruleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rule (data,list) :\n",
    "    df       = data[list].groupby(list).count().reset_index()\n",
    "    df['nn'] = df.index\n",
    "    ar       = np.zeros((df.shape[0],nClusters),dtype=np.int16)\n",
    "    return([df,ar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-07 22:05:51.270202 Read test datas \n",
      "(1385522, 100) (43831, 100) (40718, 100) (206, 100)\n",
      "2016-06-07 22:05:57.490911 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for test data (chunks) on one time if not in disk\n",
    "print(datetime.datetime.now(),'Read test datas ')\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3bTest0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "if not OK :\n",
    "\n",
    "    readerTest = pd.read_csv(fileTest,chunksize=3000000,usecols=ruleTest);\n",
    "    i = 0;\n",
    "    for chunk in readerTest :\n",
    "        i += 1; print(datetime.datetime.now(),'chunk :',i,chunk.shape)\n",
    "\n",
    "        rule1Test = rule(chunk,rule1)\n",
    "        rule2Test = rule(chunk,rule2)\n",
    "        rule3Test = rule(chunk,rule3)\n",
    "        #rule3bTest= rule(chunk,rule3b)\n",
    "        rule4Test = rule(chunk,rule4)\n",
    "\n",
    "    del chunk\n",
    "    del readerTest\n",
    "    \n",
    "    rule1Test[0].to_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2Test[0].to_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3Test[0].to_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    #rule3bTest[0].to_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "    rule4Test[0].to_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'),rule3Test[1])\n",
    "    #np.save(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'),rule3bTest[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "else :\n",
    "    rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    #rule3bdf= pd.read_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "    rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "    \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "    #rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "    \n",
    "    rule1Test = [rule1df,rule1ar]\n",
    "    rule2Test = [rule2df,rule2ar]\n",
    "    rule3Test = [rule3df,rule3ar]\n",
    "    #rule3bTest= [rule3bdf,rule3bar]\n",
    "    rule4Test = [rule4df,rule4ar]\n",
    "    del rule1df,rule2df,rule3df,rule4df\n",
    "    del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "    #del rule3bdf,rule3bar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "\n",
    "print(datetime.datetime.now(),'Done ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iGroup = 0\n",
    "def ruleApply (group,Arr) :\n",
    "    global iGroup\n",
    "    nn = group['nn'].iloc[0]; \n",
    "    if (iGroup%50000==0) : \n",
    "        print(datetime.datetime.now(),'iGroup =',iGroup,'nn =',nn, 'len(group) = ',group.shape)\n",
    "    iGroup  += 1\n",
    "    gr  = group[['hotel_cluster','cnt']].groupby(['hotel_cluster']).sum().reset_index()\n",
    "    Arr[nn][gr['hotel_cluster']] += gr['cnt']\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTop (a) :\n",
    "    b = pd.Series(a); b=np.array(b[b>0].nlargest(nTop).index); \n",
    "    b=b+1; b.resize(nTop); b=b-1 \n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 19:15:06.391212 Read test datas from pickle\n",
      "2016-06-08 19:15:13.470740 Read&Work train datas \n",
      "(1385522, 100) (43831, 100) (40718, 100) (206, 100)\n",
      "2016-06-08 19:15:21.015167 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for train data (chunks) if not in files\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(datetime.datetime.now(),'Read test datas from pickle')\n",
    "\n",
    "rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "#rule3bdf= pd.read_pickle(os.path.join(dirTemp,'rule3bTest0Empty.pkl'))\n",
    "rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "#rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1Empty.pkl.npy'))\n",
    "rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "rule1Test = [rule1df,rule1ar]\n",
    "rule2Test = [rule2df,rule2ar]\n",
    "rule3Test = [rule3df,rule3ar]\n",
    "#rule3bTest= [rule3bdf,rule3bar]\n",
    "rule4Test = [rule4df,rule4ar]\n",
    "del rule1df,rule2df,rule3df,rule4df\n",
    "del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "#del rule3bdf,rule3bar\n",
    "# ------------------------------------------------------------------------------    \n",
    "print(datetime.datetime.now(),'Read&Work train datas ')\n",
    "chunkSize = 5000000\n",
    "\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "\n",
    "if not OK :        \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=ruleTrain+['user_id']);\n",
    "    readerTrain\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (i>1) : print(datetime.datetime.now(),'break :',i); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # Estimate influence for other user groups\n",
    "        if False :\n",
    "            chunk   = chunk.merge(grps20l,how='inner',on=['user_id'])\n",
    "            print(datetime.datetime.now(),'after :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'1'); iGroup = 0;\n",
    "        grp = chunk[rule1+ruleXtrain].groupby(rule1+['hotel_cluster']).sum().reset_index()\n",
    "        grp.loc[:,'cnt'] = 1 # grp.cnt*(3+17*grp.is_booking)\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule1Test[0].shape)\n",
    "        grp = grp.merge(rule1Test[0],how='inner',on=rule1,copy=False).groupby(rule1)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule1Test[1])\n",
    "        #ruleWork(grp,rule1Test)\n",
    "        \n",
    "\n",
    "        # Rule2 --> (year==2014) : srch_destination_id+hotel_country+hotel_market += app1\n",
    "        print(datetime.datetime.now(),'2'); iGroup = 0;\n",
    "        grp = chunk.query('dty==2014')[rule2+ruleXtrain];\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule2+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule2Test[0].shape)\n",
    "        grp = grp.merge(rule2Test[0],how='inner',on=rule2,copy=False).groupby(rule2)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule2Test[1])\n",
    "        #ruleWork(grp,rule2Test,b0Add=3,b1Add=20)\n",
    "        \n",
    "        # Rule3 --> srch_destination_id += app1\n",
    "        print(datetime.datetime.now(),'3'); iGroup = 0;\n",
    "        grp = chunk[rule3+ruleXtrain]\n",
    "        #grp.loc[:,'cnt'] = grp.cnt*(3+17*grp.is_booking)\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule3+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule3Test[0].shape)\n",
    "        grp = grp.merge(rule3Test[0],how='inner',on=rule3).groupby(rule3)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule3Test[1])\n",
    "        \n",
    "        '''\n",
    "        # Rule3b --> srch_destination_id+dt0m += app1\n",
    "        print(datetime.datetime.now(),'3b'); iGroup = 0;\n",
    "        grp = chunk[rule3b+ruleXtrain]\n",
    "        #grp.loc[:,'cnt'] = grp.cnt*(3+17*grp.is_booking)\n",
    "        #grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        # cnt=cnt \n",
    "        grp = grp.groupby(rule3b+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule3bTest[0].shape)\n",
    "        grp = grp.merge(rule3bTest[0],how='inner',on=rule3b).groupby(rule3b)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule3bTest[1])\n",
    "        '''\n",
    "        \n",
    "        # Rule4 --> hotel_country += app2\n",
    "        print(datetime.datetime.now(),'4'); iGroup = 0;\n",
    "        grp = chunk[rule4+ruleXtrain]\n",
    "        grp.loc[:,'cnt'] = (1+5*grp.is_booking) # *grp.cnt\n",
    "        grp = grp.groupby(rule4+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule4Test[0].shape)\n",
    "        grp = grp.merge(rule4Test[0],how='inner',on=rule4).groupby(rule4)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule4Test[1])\n",
    "        \n",
    "    del readerTrain\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1.pkl.npy'),rule3Test[1])\n",
    "    #np.save(os.path.join(dirTemp,'rule3bTest1.pkl.npy'),rule3bTest[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "    print(datetime.datetime.now(),'Build array5 begin')\n",
    "    rule1Arr5 = np.apply_along_axis(findTop,1,rule1Test[1]); print(rule1Arr5.shape)\n",
    "    rule2Arr5 = np.apply_along_axis(findTop,1,rule2Test[1]); print(rule2Arr5.shape)\n",
    "    rule3Arr5 = np.apply_along_axis(findTop,1,rule3Test[1]); print(rule3Arr5.shape)\n",
    "    #rule3bArr5= np.apply_along_axis(findTop,1,rule3bTest[1]); print(rule3bArr5.shape)\n",
    "    rule4Arr5 = np.apply_along_axis(findTop,1,rule4Test[1]); print(rule4Arr5.shape)\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test2.pkl.npy'),rule1Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule2Test2.pkl.npy'),rule2Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule3Test2.pkl.npy'),rule3Arr5)\n",
    "    #np.save(os.path.join(dirTemp,'rule3bTest2.pkl.npy'),rule3bArr5)\n",
    "    np.save(os.path.join(dirTemp,'rule4Test2.pkl.npy'),rule4Arr5)\n",
    "\n",
    "    print(datetime.datetime.now(),'Build array5 end')\n",
    "    \n",
    "else :  \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1.pkl.npy'))\n",
    "    #rule3bar= np.load(os.path.join(dirTemp,'rule3bTest1.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "    \n",
    "    rule1Test[1] = rule1ar\n",
    "    rule2Test[1] = rule2ar\n",
    "    rule3Test[1] = rule3ar\n",
    "    #rule3bTest[1] = rule3bar\n",
    "    rule4Test[1] = rule4ar\n",
    "    del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "    #del rule3bar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "    \n",
    "    rule1Arr5 = np.load(os.path.join(dirTemp,'rule1Test2.pkl.npy'))\n",
    "    rule2Arr5 = np.load(os.path.join(dirTemp,'rule2Test2.pkl.npy'))\n",
    "    rule3Arr5 = np.load(os.path.join(dirTemp,'rule3Test2.pkl.npy'))\n",
    "    #rule3bArr5= np.load(os.path.join(dirTemp,'rule3bTest2.pkl.npy'))\n",
    "    rule4Arr5 = np.load(os.path.join(dirTemp,'rule4Test2.pkl.npy'))\n",
    "    \n",
    "# Top clusters\n",
    "rule5Arr5 = np.array([91,42,59,28])\n",
    "        \n",
    "print(datetime.datetime.now(),'Done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 2016-06-08 Compare build with is_booking = all ()rules 11, 12, 13 --> Class3-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 11:57:39.208140 EHR Rule 1 (ui+ulc+sdi+hma+hco)\n",
      "2016-06-08 11:57:45.252804 chunk : 0 (2528243, 6)\n",
      "2016-06-08 11:57:45.252804 11\n",
      "(2528240, 6)\n",
      "2016-06-08 11:57:48.778305 rule11 arr100 shape (2528240, 100)\n",
      "2016-06-08 11:58:01.598126 chunk : 0 (5000000, 10)\n",
      "2016-06-08 11:58:01.598126 11\n",
      "2016-06-08 11:58:16.166945 grp=(new groups) 3020607\n",
      "2016-06-08 11:58:41.948985 chunk : 1 (5000000, 10)\n",
      "2016-06-08 11:58:41.948985 11\n",
      "2016-06-08 11:58:48.611869 grp=(new groups) 2997176\n",
      "2016-06-08 11:59:02.607282 chunk : 2 (5000000, 10)\n",
      "2016-06-08 11:59:02.607282 11\n",
      "2016-06-08 11:59:09.371127 grp=(new groups) 3037690\n",
      "2016-06-08 11:59:25.295471 chunk : 3 (5000000, 10)\n",
      "2016-06-08 11:59:25.295471 11\n",
      "2016-06-08 11:59:32.570483 grp=(new groups) 3006540\n",
      "2016-06-08 12:00:33.889397 chunk : 4 (5000000, 10)\n",
      "2016-06-08 12:00:33.889397 11\n",
      "2016-06-08 12:00:40.605903 grp=(new groups) 3029267\n",
      "2016-06-08 12:01:08.996890 chunk : 5 (5000000, 10)\n",
      "2016-06-08 12:01:08.996890 11\n",
      "2016-06-08 12:01:15.978273 grp=(new groups) 3029416\n",
      "2016-06-08 12:01:44.194997 chunk : 6 (5000000, 10)\n",
      "2016-06-08 12:01:44.194997 11\n",
      "2016-06-08 12:04:07.795737 grp=(new groups) 3005323\n",
      "2016-06-08 12:07:52.649560 chunk : 7 (2670293, 10)\n",
      "2016-06-08 12:07:52.649560 11\n",
      "2016-06-08 12:07:57.323786 grp=(new groups) 1626309\n",
      "2016-06-08 12:08:02.922904 Build Arr5 \n",
      "(2528240, 5)\n",
      "2016-06-08 13:15:52.264529 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate EHR rules Rule1 (rule11)\n",
    "if False :\n",
    "    \n",
    "    print(datetime.datetime.now(),'EHR Rule 1 (ui+ulc+sdi+hma+hco)')\n",
    "\n",
    "    chunkSize   = 5000000\n",
    "    rule        = rule11\n",
    "    rule11add0  = ['is_booking']\n",
    "    rule11add1  = ['dt0y','dt0m']\n",
    "    rule11Xtrain= rule + rule11add0 + rule11add1 + ruleXtrain\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=chunkSize,usecols=rule11+rule11add0);\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'11'); iGroup = 0;\n",
    "        \n",
    "        grp = chunk[(chunk.is_booking>=1) & chunk.user_location_city.notnull() & \n",
    "                    chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & \n",
    "                    chunk.hotel_country.notnull()]\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    grps = grps.groupby(rule11).count().reset_index()\n",
    "    print(grps.shape)\n",
    "    \n",
    "    grps['nn'] = grps.index; grps=grps[rule11+['nn']]\n",
    "    \n",
    "    arr100     = np.zeros((len(grps),100),dtype=int); \n",
    "    print (datetime.datetime.now(),'rule11 arr100 shape',arr100.shape)\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=rule11Xtrain);\n",
    "    \n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'11'); iGroup = 0;\n",
    "        \n",
    "        #grp = chunk[(chunk.is_booking>=1) & chunk.user_location_city.notnull() & \n",
    "        grp = chunk[(chunk.is_booking>=0) & chunk.user_location_city.notnull() & \n",
    "                    chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & \n",
    "                    chunk.hotel_country.notnull()]\n",
    "        \n",
    "        grp.loc[:,'cnt'] = ((grp.loc[:,'dt0y']-2012)*12+(grp.loc[:,'dt0m']-12)) # app0\n",
    "        grp = grp[(grp.cnt>=0) & (grp.cnt<=36)].groupby(rule11+['hotel_cluster']).sum().reset_index()\n",
    "        #grp = grp.merge(grps,how='inner',on=rule11).groupby(rule11)\n",
    "        #print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        #grp.apply(ruleApply,arr100)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp = grp.merge(grps,how='inner',on=rule)\n",
    "        \n",
    "        arr100[grp.nn.values,grp.hotel_cluster.values] += grp.cnt.values;\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    print(datetime.datetime.now(),'Build Arr5 ')\n",
    "    rule11Arr5 = np.apply_along_axis(findTop,1,arr100);\n",
    "    print(rule11Arr5.shape)\n",
    "\n",
    "    grps.to_pickle(os.path.join(dirTemp,'rule11Test0Empty.pkl'))\n",
    "    #np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule11Test1.pkl.npy'),arr100)\n",
    "    np.save(os.path.join(dirTemp,'rule11Test2.pkl.npy'),rule11Arr5)\n",
    "\n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n",
    "    \n",
    "    del chunk,grp\n",
    "    del grps,arr100,rule11Arr5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 13:15:54.564682 EHR Rule 1 (ui+sdi+hma+hco)\n",
      "2016-06-08 13:16:02.869481 chunk : 0 (2528243, 5)\n",
      "2016-06-08 13:16:02.869481 12\n",
      "(2528239, 5)\n",
      "2016-06-08 13:16:05.998501 rule12 arr100 shape (2528239, 100)\n",
      "2016-06-08 13:16:30.498607 chunk : 0 (10000000, 9)\n",
      "2016-06-08 13:16:30.498607 12\n",
      "2016-06-08 13:17:22.093779 grp=(new groups) 5629115\n",
      "2016-06-08 13:18:14.496645 chunk : 1 (10000000, 9)\n",
      "2016-06-08 13:18:14.621673 12\n",
      "2016-06-08 13:28:37.331887 chunk : 2 (10000000, 9)\n",
      "2016-06-08 13:28:37.363140 12\n",
      "2016-06-08 13:29:22.107634 grp=(new groups) 5668111\n",
      "2016-06-08 13:32:09.677180 chunk : 3 (7670293, 9)\n",
      "2016-06-08 13:32:09.677180 12\n",
      "2016-06-08 13:33:43.642605 grp=(new groups) 4332138\n",
      "2016-06-08 13:35:37.648249 Build Arr5 \n",
      "(2528239, 5)\n",
      "2016-06-08 14:51:26.590479 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate EHR rules Rule2 (rule12 ui+sdi+hma+hco (==rule11 without ulc))\n",
    "if False :\n",
    "    \n",
    "    print(datetime.datetime.now(),'EHR Rule 1 (ui+sdi+hma+hco)')\n",
    "\n",
    "    chunkSize   = 10000000\n",
    "    rule        = rule12\n",
    "    ruleAdd0  = ['is_booking']\n",
    "    ruleAdd1  = ['dt0y','dt0m']\n",
    "    ruleXXtrain= rule + ruleAdd0 + ruleAdd1 + ruleXtrain\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=chunkSize,usecols=rule+ruleAdd0);\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>=1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'12'); iGroup = 0;\n",
    "        \n",
    "        grp = chunk[(chunk.is_booking>=0) & \n",
    "                    chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & \n",
    "                    chunk.hotel_country.notnull()]\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    grps = grps.groupby(rule).count().reset_index()\n",
    "    print(grps.shape)\n",
    "    \n",
    "    grps['nn'] = grps.index; grps=grps[rule+['nn']]\n",
    "    \n",
    "    arr100     = np.zeros((len(grps),100),dtype=int); \n",
    "    print (datetime.datetime.now(),'rule12 arr100 shape',arr100.shape)\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=ruleXXtrain);\n",
    "    \n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'12'); iGroup = 0;\n",
    "        \n",
    "        #grp = chunk[(chunk.is_booking>=1) &\n",
    "        grp = chunk[(chunk.is_booking>=0) &\n",
    "                    chunk.srch_destination_id.notnull() & chunk.hotel_market.notnull() & \n",
    "                    chunk.hotel_country.notnull()]\n",
    "        \n",
    "        grp.loc[:,'cnt'] = ((grp.loc[:,'dt0y']-2012)*12+(grp.loc[:,'dt0m']-12)) # app0\n",
    "        grp = grp[(grp.cnt>=0) & (grp.cnt<=36)].groupby(rule+['hotel_cluster']).sum().reset_index()\n",
    "        #grp = grp.merge(grps,how='inner',on=rule11).groupby(rule11)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        #grp.apply(ruleApply,arr100)\n",
    "        grp = grp.merge(grps,how='inner',on=rule)\n",
    "        \n",
    "        arr100[grp.nn.values,grp.hotel_cluster.values] += grp.cnt.values;\n",
    "\n",
    "        \n",
    "    print(datetime.datetime.now(),'Build Arr5 ')\n",
    "    rule11Arr5 = np.apply_along_axis(findTop,1,arr100);\n",
    "    print(rule11Arr5.shape)\n",
    "\n",
    "    grps.to_pickle(os.path.join(dirTemp,'rule12Test0Empty.pkl'))\n",
    "    #np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule12Test1.pkl.npy'),arr100)\n",
    "    np.save(os.path.join(dirTemp,'rule12Test2.pkl.npy'),rule11Arr5)\n",
    "    \n",
    "    del chunk,grp\n",
    "    del grps,arr100,rule11Arr5\n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 17:07:29.461950 EHR Rule 1 (ui+sdi)\n",
      "2016-06-08 17:07:33.963582 chunk : 0 (2528243, 3)\n",
      "2016-06-08 17:07:33.979209 12\n",
      "2016-06-08 17:07:35.647479 groups from test file : (2522004, 3)\n",
      "2016-06-08 17:07:35.772484 rule13 arr100 shape (2522004, 100)\n",
      "2016-06-08 17:07:53.259288 chunk : 0 (10000000, 7)\n",
      "2016-06-08 17:07:53.259288 13\n",
      "2016-06-08 17:08:39.158881 from chunk and test grps is rest : 378671\n",
      "2016-06-08 17:09:05.388284 chunk : 1 (10000000, 7)\n",
      "2016-06-08 17:09:05.388284 13\n",
      "2016-06-08 17:09:34.840580 from chunk and test grps is rest : 382245\n",
      "2016-06-08 17:11:08.685869 chunk : 2 (10000000, 7)\n",
      "2016-06-08 17:11:08.685869 13\n",
      "2016-06-08 17:12:11.406433 from chunk and test grps is rest : 379090\n",
      "2016-06-08 17:13:13.623687 chunk : 3 (7670293, 7)\n",
      "2016-06-08 17:13:13.639336 13\n",
      "2016-06-08 17:14:30.147214 from chunk and test grps is rest : 288006\n",
      "2016-06-08 17:14:53.657000 Build Arr5 \n",
      "(2522004, 5)\n",
      "2016-06-08 18:25:09.987906 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate EHR rules Rule1x (rule13 ui+sdi)\n",
    "if False :\n",
    "    \n",
    "    print(datetime.datetime.now(),'EHR Rule 1 (ui+sdi)')\n",
    "\n",
    "    chunkSize   = 10000000\n",
    "    rule        = rule13\n",
    "    ruleAdd0  = ['is_booking']\n",
    "    ruleAdd1  = ['dt0y','dt0m']\n",
    "    ruleXXtrain= rule + ruleAdd0 + ruleAdd1 + ruleXtrain\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=chunkSize,usecols=rule+ruleAdd0);\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>=1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'12'); iGroup = 0;\n",
    "        \n",
    "        grp = chunk[(chunk.is_booking>=1) & \n",
    "                    chunk.srch_destination_id.notnull()]\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    grps = grps.groupby(rule).count().reset_index()\n",
    "    print(datetime.datetime.now(),\"groups from test file :\",grps.shape)\n",
    "    \n",
    "    grps['nn'] = grps.index; grps=grps[rule+['nn']]\n",
    "    \n",
    "    arr100     = np.zeros((len(grps),100),dtype=int); \n",
    "    print (datetime.datetime.now(),'rule13 arr100 shape',arr100.shape)\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=ruleXXtrain);\n",
    "    \n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        print(datetime.datetime.now(),'13'); iGroup = 0;\n",
    "        \n",
    "        #grp = chunk[(chunk.is_booking>=1) &\n",
    "        grp = chunk[(chunk.is_booking>=0) &\n",
    "                    chunk.srch_destination_id.notnull()]\n",
    "        \n",
    "        grp.loc[:,'cnt'] = ((grp.loc[:,'dt0y']-2012)*12+(grp.loc[:,'dt0m']-12)) # app0\n",
    "        grp = grp[(grp.cnt>0) & (grp.cnt<=36)].groupby(rule+['hotel_cluster']).sum().reset_index()\n",
    "        grp = grp.merge(grps,how='inner',on=rule)\n",
    "        print(datetime.datetime.now(),'from chunk and test grps is rest :',len(grp))\n",
    "        \n",
    "        arr100[grp.nn.values,grp.hotel_cluster.values] += grp.cnt.values;\n",
    "\n",
    "        \n",
    "    print(datetime.datetime.now(),'Build Arr5 ')\n",
    "    rule13Arr5 = np.apply_along_axis(findTop,1,arr100);\n",
    "    print(rule13Arr5.shape)\n",
    "\n",
    "    grps.to_pickle(os.path.join(dirTemp,'rule13Test0Empty.pkl'))\n",
    "    #np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule13Test1.pkl.npy'),arr100)\n",
    "    np.save(os.path.join(dirTemp,'rule13Test2.pkl.npy'),rule13Arr5)\n",
    "    \n",
    "    del chunk,grp\n",
    "    del grps,arr100,rule13Arr5\n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if True :\n",
    "    \n",
    "    \n",
    "    rule11df   = pd.read_pickle(os.path.join(dirTemp,'rule11Test0Empty.pkl'))\n",
    "    rule12df   = pd.read_pickle(os.path.join(dirTemp,'rule12Test0Empty.pkl'))\n",
    "    rule13df   = pd.read_pickle(os.path.join(dirTemp,'rule13Test0Empty.pkl'))\n",
    "\n",
    "    rule11Test = [rule11df]\n",
    "    rule12Test = [rule12df]\n",
    "    rule13Test = [rule13df]\n",
    "    del rule11df,rule12df,rule13df\n",
    "    \n",
    "    rule11Arr5 = np.load(os.path.join(dirTemp,'rule11Test2.pkl.npy'))\n",
    "    rule12Arr5 = np.load(os.path.join(dirTemp,'rule12Test2.pkl.npy'))\n",
    "    rule13Arr5 = np.load(os.path.join(dirTemp,'rule13Test2.pkl.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2522004, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule13Test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-05 16:15:45.070508 User stats \n",
      "2016-06-05 16:16:06.045301 chunk : 0 (10000000, 5)\n",
      "2016-06-05 16:16:06.045301 1\n",
      "2016-06-05 16:16:32.635124 chunk : 1 (10000000, 5)\n",
      "2016-06-05 16:16:32.635124 1\n",
      "2016-06-05 16:17:12.123193 chunk : 2 (10000000, 5)\n",
      "2016-06-05 16:17:12.123193 1\n",
      "2016-06-05 16:17:33.376702 chunk : 3 (7670293, 5)\n",
      "2016-06-05 16:17:33.376702 1\n",
      "(1198786, 3)\n",
      "2016-06-05 16:17:34.751060 Done \n"
     ]
    }
   ],
   "source": [
    "# Calculate user_id stats\n",
    "if True :\n",
    "    \n",
    "    print(datetime.datetime.now(),'User stats ')\n",
    "\n",
    "    chunkSize = 10000000\n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=['user_id','is_booking','cnt','dt0y','dt0m']);\n",
    "    readerTrain\n",
    "    \n",
    "    grps   = pd.DataFrame()\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (ichunk>1) : print(datetime.datetime.now(),'break :',ichunk); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'1'); iGroup = 0;\n",
    "        grp = chunk[['user_id','is_booking','cnt']].groupby('user_id').sum().reset_index()\n",
    "        \n",
    "        grps = grps.append(grp)\n",
    "        \n",
    "    grps = grps.groupby('user_id').sum().reset_index()\n",
    "    print(grps.shape)    \n",
    "    \n",
    "    print(datetime.datetime.now(),'Done ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198786"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps.groupby('is_booking').count().reset_index().head(10)\n",
    "len(grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ec809bd828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps[grps.is_booking<20].is_booking.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184100, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grps20l = grps[grps.is_booking<=20][['user_id']]; grps20l.head()\n",
    "grps20g = grps[grps.is_booking>20][['user_id']]; \n",
    "grps20l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rxEstimate (rx,hcl) :\n",
    "    rxx = np.zeros(rx.shape[1],dtype=int)\n",
    "    for i in range(rx.shape[1]) :\n",
    "        xx = (hcl==pd.Series(rx[:,i])) ; xx=xx.groupby(by=xx).count(); \n",
    "        try     : rxx[i] = xx[True]\n",
    "        except  : rxx[i] = 0 \n",
    "    return(rxx)\n",
    "def estimate (rxx) : \n",
    "    res = 0.0\n",
    "    for i in range(rxx.shape[0]) : res += rxx[i]/(i+1)\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 19:21:03.583940 Validation  debug= False\n",
      "2016-06-08 19:21:05.656538 chunk : 0 (1000000, 12)\n",
      "2016-06-08 19:22:35.345331 chunk : 1 (1000000, 12)\n",
      "2016-06-08 19:24:04.830578 chunk : 2 (1000000, 12)\n",
      "2016-06-08 19:25:32.486445 chunk : 3 (145036, 12)\n",
      "----> result =  12041.31666666847 0.0038286737152352056 3145036\n",
      "2016-06-08 19:25:47.197447 Done\n"
     ]
    }
   ],
   "source": [
    "# Validation on train(is_booking==1) datas\n",
    "Validation = True\n",
    "if Validation :        \n",
    "    debugV  = True\n",
    "    debugV  = False\n",
    "    \n",
    "    print(datetime.datetime.now(),'Validation',' debug=',debugV)\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    \n",
    "    chunkSizeV   = 1000000\n",
    "    if debugV : chunkSizeV   = 10000\n",
    "    \n",
    "    readerTrain = pd.read_csv(fileValidation,chunksize=chunkSizeV,usecols=ruleValidate+['user_id']);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = 0.0; resultl = []\n",
    "    \n",
    "    r1xx   = np.zeros((nTop),dtype=int)\n",
    "    r2xx   = np.zeros((nTop),dtype=int)\n",
    "    r3xx   = np.zeros((nTop),dtype=int)\n",
    "    r4xx   = np.zeros((nTop),dtype=int)\n",
    "    #r3bxx  = np.zeros((nTop),dtype=int)\n",
    "    r11xx  = np.zeros((nTop),dtype=int)\n",
    "    r12xx  = np.zeros((nTop),dtype=int)\n",
    "    r13xx  = np.zeros((nTop),dtype=int)\n",
    "    \n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    \n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        if (ichunk>1) and debugV : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        # Estimate influence for other user groups\n",
    "        #chunk   = chunk.merge(grps20l,how='inner',on=['user_id'])\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        #chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_4\",\"_3b\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule11Test[0],how='left',on=rule11,suffixes=[\"_5\",\"_11\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule12Test[0],how='left',on=rule12,suffixes=[\"_11\",\"_12\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule13Test[0],how='left',on=rule13,suffixes=[\"_12\",\"_13\"]); #print(chunk.head())\n",
    "        \n",
    "        \n",
    "        chunk.rename(columns={'nn':'nn_13'},inplace=True)\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); rr1=rr\n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); rr2=rr\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); rr3=rr\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); rr4=rr\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        #---------------------------- add rule 3a\n",
    "        '''\n",
    "        rr  = chunk['nn_3b'].copy(); rr[chunk.nn_3b.isnull()] = 0; rr=rr.astype(int); rr3b=rr\n",
    "        r3bx = rule3bArr5[rr.values];\n",
    "        r3bx[chunk.nn_3b.isnull().values] = rz;\n",
    "        '''\n",
    "        \n",
    "        #---------------------------- add rule 11,12 (EHR)\n",
    "        \n",
    "        rr  = chunk['nn_11'].copy(); rr[chunk.nn_11.isnull()] = 0; rr=rr.astype(int); rr11=rr\n",
    "        r11x = rule11Arr5[rr.values];\n",
    "        r11x[chunk.nn_11.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_12'].copy(); rr[chunk.nn_12.isnull()] = 0; rr=rr.astype(int); rr12=rr\n",
    "        r12x = rule12Arr5[rr.values];\n",
    "        r12x[chunk.nn_12.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_13'].copy(); rr[chunk.nn_13.isnull()] = 0; rr=rr.astype(int); rr13=rr\n",
    "        r13x = rule13Arr5[rr.values];\n",
    "        r13x[chunk.nn_13.isnull().values] = rz;\n",
    "        \n",
    "        #---------------------------------------------------\n",
    "        \n",
    "        hcl = chunk['hotel_cluster']\n",
    "        \n",
    "        \n",
    "        r1xx += rxEstimate(r1x,hcl)\n",
    "        r2xx += rxEstimate(r2x,hcl)\n",
    "        r3xx += rxEstimate(r3x,hcl)\n",
    "        r4xx += rxEstimate(r4x,hcl)\n",
    "        #r3bxx+= rxEstimate(r3bx,hcl)\n",
    "        r11xx += rxEstimate(r11x,hcl)\n",
    "        r12xx += rxEstimate(r12x,hcl)\n",
    "        r13xx += rxEstimate(r13x,hcl)\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            ra, rb  = [],[]\n",
    "            nEvent += 1;\n",
    "            xEvent  = 0;\n",
    "            \n",
    "            r11l = r11x[irow].tolist();\n",
    "            if (r11x[irow][0]==-1) : r11l = r12x[irow].tolist();\n",
    "            \n",
    "            rr   = r1x[irow].tolist()+r13x[irow]+r2x[irow].tolist()+r3x[irow].tolist()+r4x[irow].tolist();\n",
    "            #rr   = r2x[irow].tolist()+r1x[irow].tolist()+r3x[irow].tolist()+r4x[irow].tolist();\n",
    "            res5 = [i for i in rr if i>-1][0:nTop]\n",
    "                \n",
    "            res = 0.0\n",
    "            if hcl[irow] in res5 : res = 1/(res5.index(hcl[irow])+1)\n",
    "            result += res\n",
    "            resultl.append([res,xEvent])\n",
    "            \n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "            \n",
    "        if (ichunk>=1) and debugV : print(datetime.datetime.now(),'break'); break\n",
    "        \n",
    "    print('----> result = ',result,result/nEvent,len(resultl))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dty</th>\n",
       "      <th>user_location_city</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "      <th>user_id</th>\n",
       "      <th>dt0y</th>\n",
       "      <th>dt0m</th>\n",
       "      <th>srch_destination_id</th>\n",
       "      <th>is_booking</th>\n",
       "      <th>cnt</th>\n",
       "      <th>hotel_country</th>\n",
       "      <th>hotel_market</th>\n",
       "      <th>hotel_cluster</th>\n",
       "      <th>nn_1</th>\n",
       "      <th>nn_2</th>\n",
       "      <th>nn_3</th>\n",
       "      <th>nn_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>35390</td>\n",
       "      <td>911.5142</td>\n",
       "      <td>93</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>14984</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1457</td>\n",
       "      <td>92</td>\n",
       "      <td>866270.0</td>\n",
       "      <td>11511.0</td>\n",
       "      <td>10486.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>10067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>501</td>\n",
       "      <td>2014</td>\n",
       "      <td>8</td>\n",
       "      <td>8267</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>675</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6058.0</td>\n",
       "      <td>5697.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>69</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6494.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>1236</td>\n",
       "      <td>82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6497.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>47725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1048</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "      <td>8803</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>1197</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6496.0</td>\n",
       "      <td>6063.0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dty  user_location_city  orig_destination_distance  user_id  dt0y  dt0m  \\\n",
       "0  2014               35390                   911.5142       93  2014    11   \n",
       "1  2014               10067                        NaN      501  2014     8   \n",
       "2  2014               47725                        NaN     1048  2015     6   \n",
       "3  2014               47725                        NaN     1048  2015     6   \n",
       "4  2014               47725                        NaN     1048  2015     6   \n",
       "\n",
       "   srch_destination_id  is_booking  cnt  hotel_country  hotel_market  \\\n",
       "0                14984           0    1             50          1457   \n",
       "1                 8267           0    1             50           675   \n",
       "2                 8803           0    1            151            69   \n",
       "3                 8803           0    1            151          1236   \n",
       "4                 8803           0    1            151          1197   \n",
       "\n",
       "   hotel_cluster      nn_1     nn_2     nn_3  nn_4  \n",
       "0             92  866270.0  11511.0  10486.0    47  \n",
       "1             98       NaN   6058.0   5697.0    47  \n",
       "2             85       NaN   6494.0   6063.0   145  \n",
       "3             82       NaN   6497.0   6063.0   145  \n",
       "4             29       NaN   6496.0   6063.0   145  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[449755  44174   3343    223     16] 497511 [  9.04010163e-01   8.87899966e-02   6.71944942e-03   4.48231295e-04\n",
      "   3.21600929e-05] \n",
      " [423413 304371 234696 191717 164568] 1318765 [ 0.32106782  0.23080003  0.17796651  0.14537617  0.12478948] \n",
      " [390229 281437 228103 179475 158187] 1237431 \n",
      " [116970 102491 124696  87057  76522] 507736\n",
      "r11 [55651 22285 12710  8155  5446] 104247 [ 0.5338379   0.21377114  0.12192197  0.07822767  0.05224131]\n",
      "r12 [122005  51045  29630  18978  12806] 234464 [ 0.52035707  0.21770933  0.12637335  0.08094206  0.05461819]\n",
      "r13 [123539  52293  30448  19670  13336] 239286 [ 0.51628177  0.21853765  0.12724522  0.08220289  0.05573247]\n",
      "0.150400594249\n",
      "0.23359775532\n",
      "0.217322785282\n",
      "0.0784886034161\n",
      "r11 0.0235794174269\n",
      "r12 0.052371377201\n",
      "r13 0.0532329465651\n"
     ]
    }
   ],
   "source": [
    "print(r1xx,r1xx.sum(),r1xx/r1xx.sum(),'\\n',r2xx,r2xx.sum(),r2xx/r2xx.sum(),'\\n',r3xx,r3xx.sum(),'\\n',r4xx,r4xx.sum())\n",
    "#print(r3bxx,r3bxx.sum(),r3bxx/r3bxx.sum())\n",
    "print('r11',r11xx,r11xx.sum(),r11xx/r11xx.sum())\n",
    "print('r12',r12xx,r12xx.sum(),r12xx/r12xx.sum())\n",
    "print('r13',r13xx,r13xx.sum(),r13xx/r13xx.sum())\n",
    "print(estimate(r1xx)/nEvent)\n",
    "print(estimate(r2xx)/nEvent)\n",
    "print(estimate(r3xx)/nEvent)\n",
    "print(estimate(r4xx)/nEvent)\n",
    "#print(estimate(r3bxx)/nEvent)\n",
    "print('r11',estimate(r11xx)/nEvent)\n",
    "print('r12',estimate(r12xx)/nEvent)\n",
    "print('r13',estimate(r13xx)/nEvent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultl\n",
    "res=pd.DataFrame(resultl); res.columns=['result','who']; res['cumres']=res['result'].cumsum()\n",
    "#res['who'].hist()\n",
    "#res['result'].hist()\n",
    "#res['cumres'].plot()\n",
    "res[['cumres']].plot()\n",
    "#res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-08 20:19:28.394332 Build result\n",
      "2016-06-08 20:19:30.942213 chunk : 0 (1000000, 10)\n",
      "2016-06-08 20:22:02.205195 chunk : 1 (1000000, 10)\n",
      "2016-06-08 20:24:41.922012 chunk : 2 (528243, 10)\n",
      "----> result =  [0, 1, 2, 3, 4] ['5 37 55 22 11', '5 35 37 41 22', '91 31 96 77 48', '1 50 51 54 10', '50 51 42 28 59'] 2528243\n",
      "2016-06-08 20:26:52.549633 Done\n"
     ]
    }
   ],
   "source": [
    "Submission = True\n",
    "if Submission :        \n",
    "    print(datetime.datetime.now(),'Build result')\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=1000000,usecols=ruleTest);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = []\n",
    "    id     = []\n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    \n",
    "    r5x    = rule5Arr5\n",
    "    \n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        #if (ichunk>3) : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE?)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule11Test[0],how='left',on=rule11,suffixes=[\"_5\",\"_11\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule12Test[0],how='left',on=rule12,suffixes=[\"_11\",\"_12\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule13Test[0],how='left',on=rule13,suffixes=[\"_12\",\"_13\"]); #print(chunk.head())\n",
    "        \n",
    "        chunk.rename(columns={'nn':'nn_13'},inplace=True)\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); \n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        #---------------------------- add rule 11,12 (EHR)\n",
    "        \n",
    "        rr  = chunk['nn_11'].copy(); rr[chunk.nn_11.isnull()] = 0; rr=rr.astype(int); rr11=rr\n",
    "        r11x = rule11Arr5[rr.values];\n",
    "        r11x[chunk.nn_11.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_12'].copy(); rr[chunk.nn_12.isnull()] = 0; rr=rr.astype(int); rr12=rr\n",
    "        r12x = rule12Arr5[rr.values];\n",
    "        r12x[chunk.nn_12.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_13'].copy(); rr[chunk.nn_13.isnull()] = 0; rr=rr.astype(int); rr13=rr\n",
    "        r13x = rule13Arr5[rr.values];\n",
    "        r13x[chunk.nn_13.isnull().values] = rz;\n",
    "        \n",
    "        #---------------------------- ---------------------\n",
    "        \n",
    "        id= id+chunk['id'].tolist()\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            nEvent += 1;\n",
    "            \n",
    "            res5  = [];\n",
    "            \n",
    "            #res5 =        [i for i in r13x[irow] if (i>-1)]\n",
    "            \n",
    "            \n",
    "            res11= r11x[irow]\n",
    "            if (r11x[irow][0]==-1) : res11 = r12x[irow]\n",
    "                \n",
    "            res5 = res5 + [i for i in r1x[irow]   if (i>-1) and (i not in res5)]\n",
    "            \n",
    "            res5 = res5 + [i for i in res11       if (i>-1) and (i not in res5)]\n",
    "            \n",
    "            res5 = res5 + [i for i in r13x[irow]  if (i>-1) and (i not in res5)]\n",
    "            \n",
    "            res5 = res5 + [i for i in r2x[irow]   if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r3x[irow]   if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r4x[irow]   if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r5x         if (i>-1) and (i not in res5)]\n",
    "                \n",
    "            result.append(res5[0:nTop])\n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "        \n",
    "    result = [x.__str__().replace(',','')[1:-1] for x in result];\n",
    "    print('----> result = ',id[0:5],result[0:5],len(result))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\Result\\\\Leak\\\\df-Leak-2016-06-08-20-27-45.csv'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testResult = pd.DataFrame({'id':id,'hotel_cluster':result})\n",
    "fileResult = os.path.join(dirResult,'df-Leak-'+datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.csv'); fileResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hotel_cluster  id\n",
      "0   5 37 55 22 11   0\n",
      "1   5 35 37 41 22   1\n",
      "2  91 31 96 77 48   2\n",
      "3   1 50 51 54 10   3\n",
      "4  50 51 42 28 59   4\n",
      "5  91 42 95 48 39   5\n",
      "6   95 21 98 91 2   6\n",
      "7  41 59 21 37 28   7\n",
      "8  88 50 51 54 10   8\n",
      "9  55 32 34 10 50   9           hotel_cluster       id\n",
      "2528233  21 59 11 68 95  2528233\n",
      "2528234  59 87 65 81 52  2528234\n",
      "2528235   82 85 36 7 78  2528235\n",
      "2528236   85 7 93 48 15  2528236\n",
      "2528237  71 80 84 92 90  2528237\n",
      "2528238   34 26 73 0 84  2528238\n",
      "2528239  57 62 46 36 82  2528239\n",
      "2528240  54 50 51 10 94  2528240\n",
      "2528241  50 47 43 15 32  2528241\n",
      "2528242  12 36 57 62 81  2528242\n"
     ]
    }
   ],
   "source": [
    "print(testResult.head(10),testResult.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   hotel_cluster\n",
      "0   0   5 37 55 22 11\n",
      "1   1   5 35 37 41 22\n",
      "2   2  91 31 96 77 48\n",
      "3   3   1 50 51 54 10\n",
      "4   4  50 51 42 28 59\n"
     ]
    }
   ],
   "source": [
    "print(testResult[['id','hotel_cluster']].head())\n",
    "testResult[['hotel_cluster']].to_csv(fileResult,header=True,index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1Test[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx=pd.DataFrame(rule1Arr5)\n",
    "xx['r0'] =rule1Test[0]['user_location_city']\n",
    "xx['r1'] =rule1Test[0]['orig_destination_distance']\n",
    "xx['r1x']=round(rule1Test[0]['orig_destination_distance'])\n",
    "xx.head()\n",
    "len(xx.groupby(['r0','r1x'])); len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx.columns\n",
    "xx[['r1',0]].query('r1>11000').plot(kind='scatter',x='r1',y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build round version ODD variable\n",
    "rule1Test0 = rule1Test[0].copy();\n",
    "rule1Test0['orig_destination_distance']=round(rule1Test0['orig_destination_distance'],-1);\n",
    "yy = rule1Test0.groupby(['user_location_city','orig_destination_distance']).count().reset_index(); yy.nn=yy.index; \n",
    "print(yy.shape,'\\n',yy.head())\n",
    "\n",
    "xx = rule1Test0.merge(yy,how='left',on=rule1,suffixes=['_l','_r']); print(len(xx))\n",
    "\n",
    "rule1Test0 = yy\n",
    "rule1Test1 = np.zeros((len(yy),nClusters),dtype=int); \n",
    "for i in range(len(xx)) : rule1Test1[xx.nn_r[i]] += rule1Test[1][xx.nn_l[i]]\n",
    "    \n",
    "print(rule1Test[1][0])\n",
    "print(rule1Test1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX = np.apply_along_axis(findTop,1,rule1Test1); print(rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change on round version\n",
    "rule1Test[0]=rule1Test0\n",
    "rule1Test[1]=rule1Test1\n",
    "rule1Arr5=rule1ArrX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print(rule1Test0.shape,rule1Test1.shape,rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX[0:10]; pd.Series(rule1ArrX[:,4]).groupby(rule1ArrX[:,0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rule1ArrX).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
