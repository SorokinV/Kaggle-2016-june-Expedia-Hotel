{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import datetime\n",
    "\n",
    "import numpy  as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables: dirs and files\n",
    "\n",
    "dirBase   = '..'\n",
    "\n",
    "dirData   = os.path.join(dirBase,'Data')\n",
    "dirPicle  = os.path.join(dirBase,'Data','Pickle')\n",
    "dirTemp   = os.path.join(dirBase,'Temp','Temp','Class3-1')\n",
    "dirResult = os.path.join(dirBase,'Result','Leak') \n",
    "\n",
    "fileTrain      = os.path.join(dirData,'trainExt.csv')\n",
    "fileValidation = os.path.join(dirData,'trainExtMin.csv')\n",
    "fileTest       = os.path.join(dirData,'testExt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "chunkSize = 4000000\n",
    "nClusters = 100\n",
    "nTop      = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rules field\n",
    "\n",
    "rule1 = ['user_location_city','orig_destination_distance']\n",
    "rule2 = ['srch_destination_id','hotel_country','hotel_market']\n",
    "rule3 = ['srch_destination_id']\n",
    "rule4 = ['hotel_country']\n",
    "\n",
    "ruleXtest    = ['cnt','is_booking']\n",
    "ruleXtrain   = ['hotel_cluster'] + ruleXtest\n",
    "\n",
    "\n",
    "ruleTrain    = rule1+rule2+ruleXtrain+['dt0y','dty']\n",
    "ruleTest     = rule1+rule2+ruleXtest+['id']\n",
    "ruleValidate = ruleTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rule (data,list) :\n",
    "    df       = data[list].groupby(list).count().reset_index()\n",
    "    df['nn'] = df.index\n",
    "    ar       = np.zeros((df.shape[0],nClusters),dtype=np.int16)\n",
    "    return([df,ar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-03 21:02:23.197840 Read test datas \n",
      "2016-06-03 21:02:29.057992 chunk : 1 (2528243, 8)\n",
      "2016-06-03 21:02:36.512708 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for test data (chunks) on one time if not in disk\n",
    "print(datetime.datetime.now(),'Read test datas ')\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test0Empty.pkl')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "if not OK :\n",
    "\n",
    "    readerTest = pd.read_csv(fileTest,chunksize=3000000,usecols=ruleTest);\n",
    "    i = 0;\n",
    "    for chunk in readerTest :\n",
    "        i += 1; print(datetime.datetime.now(),'chunk :',i,chunk.shape)\n",
    "\n",
    "        rule1Test = rule(chunk,rule1)\n",
    "        rule2Test = rule(chunk,rule2)\n",
    "        rule3Test = rule(chunk,rule3)\n",
    "        rule4Test = rule(chunk,rule4)\n",
    "\n",
    "    del chunk\n",
    "    del readerTest\n",
    "    \n",
    "    rule1Test[0].to_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2Test[0].to_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3Test[0].to_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    rule4Test[0].to_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'),rule3Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "else :\n",
    "    rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "    rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "    rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "    rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "    \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "    \n",
    "    rule1Test = [rule1df,rule1ar]\n",
    "    rule2Test = [rule2df,rule2ar]\n",
    "    rule3Test = [rule3df,rule3ar]\n",
    "    rule4Test = [rule4df,rule4ar]\n",
    "    del rule1df,rule2df,rule3df,rule4df\n",
    "    del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "\n",
    "print(datetime.datetime.now(),'Done ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rule1 = <class 'pandas.core.frame.DataFrame'> (1385522, 3) (1385522, 100) \n",
      "    user_location_city  orig_destination_distance  nn\n",
      "0                   0                     5.4003   0\n",
      "1                   0                    12.5756   1\n",
      "2                   0                    21.7732   2\n",
      "3                   0                    36.5411   3\n",
      "4                   0                    37.3479   4\n",
      "rule2 = <class 'pandas.core.frame.DataFrame'> (43831, 4) (43831, 100) \n",
      "    srch_destination_id  hotel_country  hotel_market  nn\n",
      "0                    3             17          1597   0\n",
      "1                    4              7           246   1\n",
      "2                    8             50           416   2\n",
      "3                    9            150           805   3\n",
      "4                    9            150          2101   4\n",
      "rule3 = <class 'pandas.core.frame.DataFrame'> (40718, 2) (40718, 100) \n",
      "    srch_destination_id  nn\n",
      "0                    3   0\n",
      "1                    4   1\n",
      "2                    8   2\n",
      "3                    9   3\n",
      "4                   10   4\n",
      "rule2 = <class 'pandas.core.frame.DataFrame'> (206, 2) (206, 100) \n",
      "    hotel_country  nn\n",
      "0              0   0\n",
      "1              1   1\n",
      "2              2   2\n",
      "3              3   3\n",
      "4              4   4\n"
     ]
    }
   ],
   "source": [
    "print('rule1 =',type(rule1Test[0]),rule1Test[0].shape,rule1Test[1].shape,'\\n',rule1Test[0].head())\n",
    "print('rule2 =',type(rule2Test[0]),rule2Test[0].shape,rule2Test[1].shape,'\\n',rule2Test[0].head())\n",
    "print('rule3 =',type(rule3Test[0]),rule3Test[0].shape,rule3Test[1].shape,'\\n',rule3Test[0].head())\n",
    "print('rule2 =',type(rule4Test[0]),rule4Test[0].shape,rule4Test[1].shape,'\\n',rule4Test[0].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iGroup = 0\n",
    "def ruleApply (group,Arr) :\n",
    "    global iGroup\n",
    "    nn = group['nn'].iloc[0]; \n",
    "    if (iGroup%50000==0) : \n",
    "        print(datetime.datetime.now(),'iGroup =',iGroup,'nn =',nn, 'len(group) = ',group.shape)\n",
    "    iGroup  += 1\n",
    "    gr  = group[['hotel_cluster','cnt']].groupby(['hotel_cluster']).sum().reset_index()\n",
    "    Arr[nn][gr['hotel_cluster']] += gr['cnt']\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTop (a) :\n",
    "    b = pd.Series(a); b=np.array(b[b>0].nlargest(nTop).index); \n",
    "    b=b+1; b.resize(nTop); b=b-1 \n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-03 21:03:38.781899 Read test datas from pickle\n",
      "2016-06-03 21:03:39.164132 Read&Work train datas \n",
      "2016-06-03 21:04:04.182705 chunk : 0 (10000000, 10)\n",
      "2016-06-03 21:04:04.182705 1\n",
      "2016-06-03 21:04:15.201465 grp=(train group) (3336969, 5)\n",
      "2016-06-03 21:04:15.201465 grpx=(test group) (1385522, 3)\n",
      "2016-06-03 21:04:25.224843 grp=(new groups) 328717\n",
      "2016-06-03 21:04:30.633445 iGroup = 0 nn = 8 len(group) =  (1, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3-64\\lib\\site-packages\\ipykernel\\__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-03 21:07:19.280415 iGroup = 50000 nn = 184358 len(group) =  (1, 6)\n",
      "2016-06-03 21:10:32.696961 iGroup = 100000 nn = 423886 len(group) =  (1, 6)\n",
      "2016-06-03 21:13:54.508641 iGroup = 150000 nn = 628144 len(group) =  (1, 6)\n",
      "2016-06-03 21:17:25.761983 iGroup = 200000 nn = 849219 len(group) =  (1, 6)\n",
      "2016-06-03 21:20:50.690135 iGroup = 250000 nn = 1061586 len(group) =  (1, 6)\n",
      "2016-06-03 21:24:13.426919 iGroup = 300000 nn = 1249785 len(group) =  (1, 6)\n",
      "2016-06-03 21:26:11.151041 2\n",
      "2016-06-03 21:26:25.382893 grp=(train group) (223240, 6)\n",
      "2016-06-03 21:26:25.382893 grpx=(test group) (43831, 4)\n",
      "2016-06-03 21:26:26.398727 grp=(new groups) 28702\n",
      "2016-06-03 21:26:27.851927 iGroup = 0 nn = 0 len(group) =  (2, 7)\n",
      "2016-06-03 21:28:30.213690 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3-64\\lib\\site-packages\\pandas\\core\\indexing.py:545: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-03 21:28:33.583507 grp=(train group) (251354, 4)\n",
      "2016-06-03 21:28:33.583507 grpx=(test group) (40718, 2)\n",
      "2016-06-03 21:28:33.652310 grp=(new groups) 28596\n",
      "2016-06-03 21:28:33.864420 iGroup = 0 nn = 0 len(group) =  (2, 5)\n",
      "2016-06-03 21:30:56.236984 4\n",
      "2016-06-03 21:30:58.315207 grp=(train group) (6637, 4)\n",
      "2016-06-03 21:30:58.315207 grpx=(test group) (206, 2)\n",
      "2016-06-03 21:30:58.330833 grp=(new groups) 204\n",
      "2016-06-03 21:30:58.330833 iGroup = 0 nn = 0 len(group) =  (43, 5)\n",
      "2016-06-03 21:31:34.836349 chunk : 1 (10000000, 10)\n",
      "2016-06-03 21:31:34.836349 1\n",
      "2016-06-03 21:31:45.681395 grp=(train group) (3370737, 5)\n",
      "2016-06-03 21:31:45.681395 grpx=(test group) (1385522, 3)\n",
      "2016-06-03 21:32:03.511485 grp=(new groups) 329508\n",
      "2016-06-03 21:32:08.574472 iGroup = 0 nn = 0 len(group) =  (1, 6)\n",
      "2016-06-03 21:37:55.519847 iGroup = 50000 nn = 184540 len(group) =  (1, 6)\n",
      "2016-06-03 21:41:27.517500 iGroup = 100000 nn = 423783 len(group) =  (1, 6)\n",
      "2016-06-03 21:45:10.967888 iGroup = 150000 nn = 627863 len(group) =  (1, 6)\n",
      "2016-06-03 21:49:12.863111 iGroup = 200000 nn = 844490 len(group) =  (1, 6)\n",
      "2016-06-03 21:53:28.574802 iGroup = 250000 nn = 1059609 len(group) =  (1, 6)\n",
      "2016-06-03 21:57:31.531653 iGroup = 300000 nn = 1245710 len(group) =  (1, 6)\n",
      "2016-06-03 22:00:07.808085 2\n",
      "2016-06-03 22:00:16.208673 grp=(train group) (225416, 6)\n",
      "2016-06-03 22:00:16.208673 grpx=(test group) (43831, 4)\n",
      "2016-06-03 22:00:16.455837 grp=(new groups) 28875\n",
      "2016-06-03 22:00:19.365511 iGroup = 0 nn = 0 len(group) =  (1, 7)\n",
      "2016-06-03 22:02:47.395778 3\n",
      "2016-06-03 22:02:50.605037 grp=(train group) (252592, 4)\n",
      "2016-06-03 22:02:50.605037 grpx=(test group) (40718, 2)\n",
      "2016-06-03 22:02:50.683171 grp=(new groups) 28768\n",
      "2016-06-03 22:02:50.874882 iGroup = 0 nn = 0 len(group) =  (1, 5)\n",
      "2016-06-03 22:05:16.510368 4\n",
      "2016-06-03 22:05:21.921337 grp=(train group) (6633, 4)\n",
      "2016-06-03 22:05:21.921337 grpx=(test group) (206, 2)\n",
      "2016-06-03 22:05:21.936947 grp=(new groups) 204\n",
      "2016-06-03 22:05:21.952578 iGroup = 0 nn = 0 len(group) =  (45, 5)\n",
      "2016-06-03 22:06:00.871614 chunk : 2 (10000000, 10)\n",
      "2016-06-03 22:06:01.669066 1\n",
      "2016-06-03 22:06:24.504868 grp=(train group) (3362432, 5)\n",
      "2016-06-03 22:06:24.504868 grpx=(test group) (1385522, 3)\n",
      "2016-06-03 22:06:46.458383 grp=(new groups) 329149\n",
      "2016-06-03 22:06:56.429273 iGroup = 0 nn = 9 len(group) =  (1, 6)\n",
      "2016-06-03 22:13:37.041623 iGroup = 50000 nn = 186666 len(group) =  (1, 6)\n",
      "2016-06-03 22:17:08.766920 iGroup = 100000 nn = 425199 len(group) =  (1, 6)\n",
      "2016-06-03 22:21:08.391543 iGroup = 150000 nn = 628542 len(group) =  (3, 6)\n",
      "2016-06-03 22:25:19.784226 iGroup = 200000 nn = 844558 len(group) =  (1, 6)\n",
      "2016-06-03 22:29:32.140763 iGroup = 250000 nn = 1060063 len(group) =  (1, 6)\n",
      "2016-06-03 22:34:02.318576 iGroup = 300000 nn = 1246900 len(group) =  (1, 6)\n",
      "2016-06-03 22:36:29.915755 2\n",
      "2016-06-03 22:36:38.964208 grp=(train group) (225819, 6)\n",
      "2016-06-03 22:36:38.964208 grpx=(test group) (43831, 4)\n",
      "2016-06-03 22:36:39.199558 grp=(new groups) 28910\n",
      "2016-06-03 22:36:42.346822 iGroup = 0 nn = 0 len(group) =  (2, 7)\n",
      "2016-06-03 22:39:10.259635 3\n",
      "2016-06-03 22:39:15.562076 grp=(train group) (253969, 4)\n",
      "2016-06-03 22:39:15.562076 grpx=(test group) (40718, 2)\n",
      "2016-06-03 22:39:15.733957 grp=(new groups) 28802\n",
      "2016-06-03 22:39:16.289608 iGroup = 0 nn = 0 len(group) =  (2, 5)\n",
      "2016-06-03 22:43:44.408401 4\n",
      "2016-06-03 22:43:48.301233 grp=(train group) (6647, 4)\n",
      "2016-06-03 22:43:48.301233 grpx=(test group) (206, 2)\n",
      "2016-06-03 22:43:48.301233 grp=(new groups) 205\n",
      "2016-06-03 22:43:48.316866 iGroup = 0 nn = 0 len(group) =  (46, 5)\n",
      "2016-06-03 22:44:26.182574 chunk : 3 (7670293, 10)\n",
      "2016-06-03 22:44:26.182574 1\n",
      "2016-06-03 22:44:38.029007 grp=(train group) (2672866, 5)\n",
      "2016-06-03 22:44:38.029007 grpx=(test group) (1385522, 3)\n",
      "2016-06-03 22:44:54.169908 grp=(new groups) 287830\n",
      "2016-06-03 22:45:03.280896 iGroup = 0 nn = 22 len(group) =  (1, 6)\n",
      "2016-06-03 22:51:01.037407 iGroup = 50000 nn = 232091 len(group) =  (1, 6)\n",
      "2016-06-03 22:54:33.548387 iGroup = 100000 nn = 496605 len(group) =  (1, 6)\n",
      "2016-06-03 22:58:29.846800 iGroup = 150000 nn = 705257 len(group) =  (1, 6)\n",
      "2016-06-03 23:02:46.213253 iGroup = 200000 nn = 962240 len(group) =  (1, 6)\n",
      "2016-06-03 23:06:51.684491 iGroup = 250000 nn = 1206163 len(group) =  (1, 6)\n",
      "2016-06-03 23:10:21.537719 2\n",
      "2016-06-03 23:10:24.582279 grp=(train group) (202255, 6)\n",
      "2016-06-03 23:10:24.582279 grpx=(test group) (43831, 4)\n",
      "2016-06-03 23:10:24.660416 grp=(new groups) 26967\n",
      "2016-06-03 23:10:25.582071 iGroup = 0 nn = 1 len(group) =  (23, 7)\n",
      "2016-06-03 23:12:52.319467 3\n",
      "2016-06-03 23:12:54.492566 grp=(train group) (228675, 4)\n",
      "2016-06-03 23:12:54.492566 grpx=(test group) (40718, 2)\n",
      "2016-06-03 23:12:54.555192 grp=(new groups) 27132\n",
      "2016-06-03 23:12:54.743656 iGroup = 0 nn = 1 len(group) =  (24, 5)\n",
      "2016-06-03 23:15:22.180707 4\n",
      "2016-06-03 23:15:25.975594 grp=(train group) (6549, 4)\n",
      "2016-06-03 23:15:25.975594 grpx=(test group) (206, 2)\n",
      "2016-06-03 23:15:25.991220 grp=(new groups) 205\n",
      "2016-06-03 23:15:26.006845 iGroup = 0 nn = 0 len(group) =  (44, 5)\n",
      "2016-06-03 23:15:33.960507 Build array5 begin\n",
      "(1385522, 5)\n",
      "(43831, 5)\n",
      "(40718, 5)\n",
      "(206, 5)\n",
      "2016-06-04 00:09:10.911713 Build array5 end\n",
      "2016-06-04 00:09:10.911713 Done \n"
     ]
    }
   ],
   "source": [
    "# Reader for train data (chunks) if not in files\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "print(datetime.datetime.now(),'Read test datas from pickle')\n",
    "\n",
    "rule1df = pd.read_pickle(os.path.join(dirTemp,'rule1Test0Empty.pkl'))\n",
    "rule2df = pd.read_pickle(os.path.join(dirTemp,'rule2Test0Empty.pkl'))\n",
    "rule3df = pd.read_pickle(os.path.join(dirTemp,'rule3Test0Empty.pkl'))\n",
    "rule4df = pd.read_pickle(os.path.join(dirTemp,'rule4Test0Empty.pkl'))\n",
    "\n",
    "rule1ar = np.load(os.path.join(dirTemp,'rule1Test1Empty.pkl.npy'))\n",
    "rule2ar = np.load(os.path.join(dirTemp,'rule2Test1Empty.pkl.npy'))\n",
    "rule3ar = np.load(os.path.join(dirTemp,'rule3Test1Empty.pkl.npy'))\n",
    "rule4ar = np.load(os.path.join(dirTemp,'rule4Test1Empty.pkl.npy'))\n",
    "\n",
    "rule1Test = [rule1df,rule1ar]\n",
    "rule2Test = [rule2df,rule2ar]\n",
    "rule3Test = [rule3df,rule3ar]\n",
    "rule4Test = [rule4df,rule4ar]\n",
    "del rule1df,rule2df,rule3df,rule4df\n",
    "del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "# ------------------------------------------------------------------------------    \n",
    "print(datetime.datetime.now(),'Read&Work train datas ')\n",
    "chunkSize = 10000000\n",
    "#chunkSize = 2000000\n",
    "#chunkSize = 60000\n",
    "\n",
    "OK =    os.path.exists(os.path.join(dirTemp,'rule1Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule2Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule3Test1.pkl.npy')) and \\\n",
    "        os.path.exists(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "\n",
    "if not OK :        \n",
    "    readerTrain = pd.read_csv(fileTrain,chunksize=chunkSize,usecols=ruleTrain);\n",
    "    readerTrain\n",
    "\n",
    "    ichunk = -1;\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1; \n",
    "        #if (i>1) : print(datetime.datetime.now(),'break :',i); break;\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        #\n",
    "        # app1 = 3+is_booking*17\n",
    "        # app2 = 1+is_booking*5\n",
    "        #\n",
    "        \n",
    "        # Rule1 --> user_location_city+orig_destination_distance += 1\n",
    "        print(datetime.datetime.now(),'1'); iGroup = 0;\n",
    "        grp = chunk[rule1+ruleXtrain].groupby(rule1+['hotel_cluster']).sum().reset_index()\n",
    "        grp.loc[:,'cnt'] = 1 # grp.cnt*(3+17*grp.is_booking)\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule1Test[0].shape)\n",
    "        grp = grp.merge(rule1Test[0],how='inner',on=rule1,copy=False).groupby(rule1)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule1Test[1])\n",
    "        #ruleWork(grp,rule1Test)\n",
    "        \n",
    "\n",
    "        # Rule2 --> (year==2014) : srch_destination_id+hotel_country+hotel_market += app1\n",
    "        print(datetime.datetime.now(),'2'); iGroup = 0;\n",
    "        grp = chunk.query('dty==2014')[rule2+ruleXtrain];\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule2+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule2Test[0].shape)\n",
    "        grp = grp.merge(rule2Test[0],how='inner',on=rule2,copy=False).groupby(rule2)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp));\n",
    "        grp.apply(ruleApply,rule2Test[1])\n",
    "        #ruleWork(grp,rule2Test,b0Add=3,b1Add=20)\n",
    "        \n",
    "        # Rule3 --> srch_destination_id += app1\n",
    "        print(datetime.datetime.now(),'3'); iGroup = 0;\n",
    "        grp = chunk[rule3+ruleXtrain]\n",
    "        #grp.loc[:,'cnt'] = grp.cnt*(3+17*grp.is_booking)\n",
    "        grp.loc[:,'cnt'] = (3+17*grp.is_booking) #*grp.cnt\n",
    "        grp = grp.groupby(rule3+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule3Test[0].shape)\n",
    "        grp = grp.merge(rule3Test[0],how='inner',on=rule3).groupby(rule3)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule3Test[1])\n",
    "        \n",
    "        # Rule4 --> hotel_country += app2\n",
    "        print(datetime.datetime.now(),'4'); iGroup = 0;\n",
    "        grp = chunk[rule4+ruleXtrain]\n",
    "        grp.loc[:,'cnt'] = (1+5*grp.is_booking) # *grp.cnt\n",
    "        grp = grp.groupby(rule4+['hotel_cluster']).sum().reset_index()\n",
    "        print(datetime.datetime.now(),'grp=(train group)',grp.shape)\n",
    "        print(datetime.datetime.now(),'grpx=(test group)',rule4Test[0].shape)\n",
    "        grp = grp.merge(rule4Test[0],how='inner',on=rule4).groupby(rule4)\n",
    "        print(datetime.datetime.now(),'grp=(new groups)',len(grp))\n",
    "        grp.apply(ruleApply,rule4Test[1])\n",
    "        \n",
    "    del readerTrain\n",
    "    np.save(os.path.join(dirTemp,'rule1Test1.pkl.npy'),rule1Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule2Test1.pkl.npy'),rule2Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule3Test1.pkl.npy'),rule3Test[1])\n",
    "    np.save(os.path.join(dirTemp,'rule4Test1.pkl.npy'),rule4Test[1])\n",
    "    \n",
    "    print(datetime.datetime.now(),'Build array5 begin')\n",
    "    rule1Arr5 = np.apply_along_axis(findTop,1,rule1Test[1]); print(rule1Arr5.shape)\n",
    "    rule2Arr5 = np.apply_along_axis(findTop,1,rule2Test[1]); print(rule2Arr5.shape)\n",
    "    rule3Arr5 = np.apply_along_axis(findTop,1,rule3Test[1]); print(rule3Arr5.shape)\n",
    "    rule4Arr5 = np.apply_along_axis(findTop,1,rule4Test[1]); print(rule4Arr5.shape)\n",
    "\n",
    "    np.save(os.path.join(dirTemp,'rule1Test2.pkl.npy'),rule1Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule2Test2.pkl.npy'),rule2Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule3Test2.pkl.npy'),rule3Arr5)\n",
    "    np.save(os.path.join(dirTemp,'rule4Test2.pkl.npy'),rule4Arr5)\n",
    "\n",
    "    print(datetime.datetime.now(),'Build array5 end')\n",
    "    \n",
    "else :  \n",
    "    rule1ar = np.load(os.path.join(dirTemp,'rule1Test1.pkl.npy'))\n",
    "    rule2ar = np.load(os.path.join(dirTemp,'rule2Test1.pkl.npy'))\n",
    "    rule3ar = np.load(os.path.join(dirTemp,'rule3Test1.pkl.npy'))\n",
    "    rule4ar = np.load(os.path.join(dirTemp,'rule4Test1.pkl.npy'))\n",
    "    \n",
    "    rule1Test[1] = rule1ar\n",
    "    rule2Test[1] = rule2ar\n",
    "    rule3Test[1] = rule3ar\n",
    "    rule4Test[1] = rule4ar\n",
    "    del rule1ar,rule2ar,rule3ar,rule4ar\n",
    "    \n",
    "    print(rule1Test[1].shape,rule2Test[1].shape,rule3Test[1].shape,rule4Test[1].shape)\n",
    "    \n",
    "    rule1Arr5 = np.load(os.path.join(dirTemp,'rule1Test2.pkl.npy'))\n",
    "    rule2Arr5 = np.load(os.path.join(dirTemp,'rule2Test2.pkl.npy'))\n",
    "    rule3Arr5 = np.load(os.path.join(dirTemp,'rule3Test2.pkl.npy'))\n",
    "    rule4Arr5 = np.load(os.path.join(dirTemp,'rule4Test2.pkl.npy'))\n",
    "        \n",
    "print(datetime.datetime.now(),'Done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame(rule4Arr5); df.head();\n",
    "df.hist(bins=100); \n",
    "#len(df[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top clusters\n",
    "rule5Arr5 = np.array([91,42,59,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-04 05:38:23.348268 Validation\n",
      "2016-06-04 05:38:28.812493 chunk : 0 (1000000, 10)\n",
      "2016-06-04 05:39:44.224128 chunk : 1 (1000000, 10)\n",
      "2016-06-04 05:40:57.911477 chunk : 2 (1000000, 10)\n",
      "2016-06-04 05:42:20.056675 chunk : 3 (145036, 10)\n",
      "----> result =  1086198.9333327846 0.3453693163870889 3145036\n",
      "2016-06-04 05:42:29.488974 Done\n"
     ]
    }
   ],
   "source": [
    "# Validation on train(is_booking==1) datas\n",
    "Validation = True\n",
    "if Validation :        \n",
    "    print(datetime.datetime.now(),'Validation')\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    readerTrain = pd.read_csv(fileValidation,chunksize=1000000,usecols=ruleValidate);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = 0.0; resultl = []\n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        if (ichunk>3) : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); \n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        hcl = chunk['hotel_cluster']\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            ra, rb  = [],[]\n",
    "            nEvent += 1;\n",
    "            xEvent  = 0;\n",
    "            \n",
    "            rr   = r1x[irow].tolist()+r2x[irow].tolist()+r3x[irow].tolist()+r4x[irow].tolist();\n",
    "            res5 = [i for i in rr if i>-1][0:nTop]\n",
    "                \n",
    "            res = 0.0\n",
    "            if hcl[irow] in res5 : res = 1/(res5.index(hcl[irow])+1)\n",
    "            result += res\n",
    "            resultl.append([res,xEvent])\n",
    "            \n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "        \n",
    "    print('----> result = ',result,result/nEvent,len(resultl))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultl\n",
    "res=pd.DataFrame(resultl); res.columns=['result','who']; res['cumres']=res['result'].cumsum()\n",
    "#res['who'].hist()\n",
    "#res['result'].hist()\n",
    "#res['cumres'].plot()\n",
    "res[['cumres']].plot()\n",
    "#res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-04 05:45:04.482374 Build result\n",
      "2016-06-04 05:45:08.838331 chunk : 0 (1000000, 8)\n",
      "2016-06-04 05:47:22.874867 chunk : 1 (1000000, 8)\n",
      "2016-06-04 05:49:37.774571 chunk : 2 (528243, 8)\n",
      "----> result =  [0, 1, 2, 3, 4] ['5 37 55 22 11', '5 22 58 99 75', '91 96 77 48 59', '1 10 19 94 34', '50 51 91 2 59'] 2528243\n",
      "2016-06-04 05:51:50.436253 Done\n"
     ]
    }
   ],
   "source": [
    "Submission = True\n",
    "if Submission :        \n",
    "    print(datetime.datetime.now(),'Build result')\n",
    "    #readerTrain = pd.read_csv(fileValidation,chunksize=chunkSize);\n",
    "    readerTrain = pd.read_csv(fileTest,chunksize=1000000,usecols=ruleTest);\n",
    "    #readerTrain\n",
    "    \n",
    "    result = []\n",
    "    id     = []\n",
    "    nEvent = 0;  \n",
    "    ichunk = -1\n",
    "    \n",
    "    r5x    = rule5Arr5\n",
    "    \n",
    "    for chunk in readerTrain :\n",
    "        ichunk += 1\n",
    "        irow = -1;\n",
    "        #if (ichunk>3) : print(datetime.datetime.now(),'break'); break\n",
    "        print(datetime.datetime.now(),'chunk :',ichunk,chunk.shape)\n",
    "        \n",
    "        # for round estimate (EFFECT IS NEGATIVE?)\n",
    "        #chunk['orig_destination_distance']=round(chunk['orig_destination_distance'],-1);\n",
    "        \n",
    "        chunk   = chunk.merge(rule1Test[0],how='left',on=rule1,suffixes=[\"_0\",\"_1\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule2Test[0],how='left',on=rule2,suffixes=[\"_1\",\"_2\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule3Test[0],how='left',on=rule3,suffixes=[\"_2\",\"_3\"]); #print(chunk.head())\n",
    "        chunk   = chunk.merge(rule4Test[0],how='left',on=rule4,suffixes=[\"_3\",\"_4\"]); #print(chunk.head())\n",
    "        \n",
    "        rz  = np.array([-1]*nTop)\n",
    "        \n",
    "        rr  = chunk['nn_1'].copy(); rr[chunk.nn_1.isnull()] = 0; rr=rr.astype(int); \n",
    "        #print(rr.values[0:5],chunk.nn_1.isnull().tolist()[0:5])\n",
    "        r1x = rule1Arr5[rr.values];\n",
    "        r1x[chunk.nn_1.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_2'].copy(); rr[chunk.nn_2.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r2x = rule2Arr5[rr.values];\n",
    "        r2x[chunk.nn_2.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_3'].copy(); rr[chunk.nn_3.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r3x = rule3Arr5[rr.values];\n",
    "        r3x[chunk.nn_3.isnull().values] = rz;\n",
    "        \n",
    "        rr  = chunk['nn_4'].copy(); rr[chunk.nn_4.isnull()] = 0; rr=rr.astype(int); #print(rr.values[0:5])\n",
    "        r4x = rule4Arr5[rr.values];\n",
    "        r4x[chunk.nn_4.isnull().values] = rz;\n",
    "        \n",
    "        id= id+chunk['id'].tolist()\n",
    "        \n",
    "        for irow in range(chunk.shape[0]):\n",
    "            #if (irow>25000) : break;\n",
    "            nEvent += 1;\n",
    "            \n",
    "            res5 =        [i for i in r1x[irow] if (i>-1)]\n",
    "            res5 = res5 + [i for i in r2x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r3x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r4x[irow] if (i>-1) and (i not in res5)]\n",
    "            res5 = res5 + [i for i in r5x       if (i>-1) and (i not in res5)]\n",
    "            \n",
    "            \n",
    "                \n",
    "            result.append(res5[0:nTop])\n",
    "            \n",
    "            #if (nEvent%10000==0) : print(ichunk,irow,result,result/nEvent,hcl[irow],res5) #,ra,rb)\n",
    "        \n",
    "    result = [x.__str__().replace(',','')[1:-1] for x in result];\n",
    "    print('----> result = ',id[0:5],result[0:5],len(result))\n",
    "    print(datetime.datetime.now(),'Done')\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\Result\\\\Leak\\\\df-Leak-2016-06-04-05-55-32.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testResult = pd.DataFrame({'id':id,'hotel_cluster':result})\n",
    "fileResult = os.path.join(dirResult,'df-Leak-'+datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.csv'); fileResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    hotel_cluster  id\n",
      "0   5 37 55 22 11   0\n",
      "1   5 22 58 99 75   1\n",
      "2  91 96 77 48 59   2\n",
      "3   1 10 19 94 34   3\n",
      "4   50 51 91 2 59   4\n",
      "5  91 42 95 48 39   5\n",
      "6   95 21 98 91 2   6\n",
      "7  41 59 21 37 28   7\n",
      "8  88 10 19 94 34   8\n",
      "9  55 32 34 10 50   9           hotel_cluster       id\n",
      "2528233   21 59 95 4 19  2528233\n",
      "2528234  59 87 81 65 52  2528234\n",
      "2528235   82 85 36 7 61  2528235\n",
      "2528236   93 85 7 48 15  2528236\n",
      "2528237  71 80 84 92 90  2528237\n",
      "2528238   34 26 73 0 84  2528238\n",
      "2528239  57 62 46 36 82  2528239\n",
      "2528240  54 10 19 94 34  2528240\n",
      "2528241  50 47 43 15 32  2528241\n",
      "2528242  12 36 57 81 62  2528242\n"
     ]
    }
   ],
   "source": [
    "print(testResult.head(10),testResult.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   hotel_cluster\n",
      "0   0   5 37 55 22 11\n",
      "1   1   5 22 58 99 75\n",
      "2   2  91 96 77 48 59\n",
      "3   3   1 10 19 94 34\n",
      "4   4   50 51 91 2 59\n"
     ]
    }
   ],
   "source": [
    "print(testResult[['id','hotel_cluster']].head())\n",
    "testResult[['hotel_cluster']].to_csv(fileResult,header=True,index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1Test[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx=pd.DataFrame(rule1Arr5)\n",
    "xx['r0'] =rule1Test[0]['user_location_city']\n",
    "xx['r1'] =rule1Test[0]['orig_destination_distance']\n",
    "xx['r1x']=round(rule1Test[0]['orig_destination_distance'])\n",
    "xx.head()\n",
    "len(xx.groupby(['r0','r1x'])); len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xx.columns\n",
    "xx[['r1',0]].query('r1>11000').plot(kind='scatter',x='r1',y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build round version ODD variable\n",
    "rule1Test0 = rule1Test[0].copy();\n",
    "rule1Test0['orig_destination_distance']=round(rule1Test0['orig_destination_distance'],-1);\n",
    "yy = rule1Test0.groupby(['user_location_city','orig_destination_distance']).count().reset_index(); yy.nn=yy.index; \n",
    "print(yy.shape,'\\n',yy.head())\n",
    "\n",
    "xx = rule1Test0.merge(yy,how='left',on=rule1,suffixes=['_l','_r']); print(len(xx))\n",
    "\n",
    "rule1Test0 = yy\n",
    "rule1Test1 = np.zeros((len(yy),nClusters),dtype=int); \n",
    "for i in range(len(xx)) : rule1Test1[xx.nn_r[i]] += rule1Test[1][xx.nn_l[i]]\n",
    "    \n",
    "print(rule1Test[1][0])\n",
    "print(rule1Test1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX = np.apply_along_axis(findTop,1,rule1Test1); print(rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change on round version\n",
    "rule1Test[0]=rule1Test0\n",
    "rule1Test[1]=rule1Test1\n",
    "rule1Arr5=rule1ArrX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " print(rule1Test0.shape,rule1Test1.shape,rule1ArrX.shape,'\\n',rule1ArrX[0:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rule1ArrX[0:10]; pd.Series(rule1ArrX[:,4]).groupby(rule1ArrX[:,0]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(rule1ArrX).hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
